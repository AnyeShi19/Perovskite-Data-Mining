{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this report, we are trying to achive crystal system and space group classification using pure X-ray diffraction data via machine learning approach. Necessary python libraries are imported first. Notebly, here we use sklearn based model to establish original machine learning model. In addition, GridSearchCV is involved to optimize hyperparameters within each model. To screen and evaluate best model with highest accuracy, 3 types of classification models are selected. Linear-based supervised machine learning model: LogisticRegression and SVM model. Clustter ensembling model: RandomForest and Adaboost. Neural Network model: Multi Layer Perceptron. What's more, we are trying to employ volting classifier to merge multiple models together to study synergistic effect of multi-model based classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data processing\n",
    "import time  \n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "from sklearn.metrics import classification_report, cohen_kappa_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, f1_score,accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder,MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier,VotingClassifier,AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part1 Data Preparation and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H26C25N4ClO3F</td>\n",
       "      <td>2</td>\n",
       "      <td>2227539</td>\n",
       "      <td>aP</td>\n",
       "      <td>4.909847048222291e-137</td>\n",
       "      <td>2.993258003847605e-111</td>\n",
       "      <td>3.522733828136009e-88</td>\n",
       "      <td>8.003408706033912e-68</td>\n",
       "      <td>3.510182114016871e-50</td>\n",
       "      <td>2.971965685119351e-35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023087572208346268</td>\n",
       "      <td>0.03470161046711315</td>\n",
       "      <td>0.026240115052367056</td>\n",
       "      <td>0.018112677152580716</td>\n",
       "      <td>0.013301584321419636</td>\n",
       "      <td>0.01943209584729744</td>\n",
       "      <td>0.03018274253498323</td>\n",
       "      <td>0.01792259535015317</td>\n",
       "      <td>0.022856899978009926</td>\n",
       "      <td>0.006056703830570913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CoP3H36C23</td>\n",
       "      <td>2</td>\n",
       "      <td>4085709</td>\n",
       "      <td>aP</td>\n",
       "      <td>3.3916256736702194e-189</td>\n",
       "      <td>1.2351214695284974e-158</td>\n",
       "      <td>8.683022289037302e-131</td>\n",
       "      <td>1.1783970561129091e-105</td>\n",
       "      <td>3.087250502676185e-83</td>\n",
       "      <td>1.5613906675494182e-63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013113304862799295</td>\n",
       "      <td>0.01394778016454681</td>\n",
       "      <td>0.01540255338368422</td>\n",
       "      <td>0.01378661526772984</td>\n",
       "      <td>0.02263673724886634</td>\n",
       "      <td>0.021858884972270273</td>\n",
       "      <td>0.020877693697405397</td>\n",
       "      <td>0.028194846009904313</td>\n",
       "      <td>0.02273571935202447</td>\n",
       "      <td>0.014079356216551837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H34C34S2(N2O)3</td>\n",
       "      <td>2</td>\n",
       "      <td>2210818</td>\n",
       "      <td>aP</td>\n",
       "      <td>6.144994743224586e-218</td>\n",
       "      <td>5.954486105173294e-185</td>\n",
       "      <td>1.113849578233596e-154</td>\n",
       "      <td>4.02224287840651e-127</td>\n",
       "      <td>2.803945229831649e-102</td>\n",
       "      <td>3.773377444235225e-80</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015220210645994606</td>\n",
       "      <td>0.012784898045865972</td>\n",
       "      <td>0.009240284757519177</td>\n",
       "      <td>0.013426332226241806</td>\n",
       "      <td>0.016861790459496533</td>\n",
       "      <td>0.02535214160778556</td>\n",
       "      <td>0.009457101150023196</td>\n",
       "      <td>0.008136538693339681</td>\n",
       "      <td>0.0067923851135896175</td>\n",
       "      <td>0.004124100972046004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P2H44Ru2C56N14Cl4O9.25F12</td>\n",
       "      <td>2</td>\n",
       "      <td>4312692</td>\n",
       "      <td>aP</td>\n",
       "      <td>4.146075234670375e-83</td>\n",
       "      <td>1.3978668895159648e-63</td>\n",
       "      <td>9.098167871772325e-47</td>\n",
       "      <td>1.143145632965874e-32</td>\n",
       "      <td>2.772736921587006e-21</td>\n",
       "      <td>1.2983005990880624e-12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006725776040262346</td>\n",
       "      <td>0.009491708355898012</td>\n",
       "      <td>0.011158401538297549</td>\n",
       "      <td>0.009486048490128369</td>\n",
       "      <td>0.009029170189152664</td>\n",
       "      <td>0.0137523993119171</td>\n",
       "      <td>0.0116070571917301</td>\n",
       "      <td>0.009644470200327366</td>\n",
       "      <td>0.0073570053747198765</td>\n",
       "      <td>0.0037940031355739533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>H22C23SN2O3</td>\n",
       "      <td>2</td>\n",
       "      <td>2208064</td>\n",
       "      <td>aP</td>\n",
       "      <td>1.7606859769070014e-259</td>\n",
       "      <td>2.249971724353988e-223</td>\n",
       "      <td>5.550494842694082e-190</td>\n",
       "      <td>2.643296218774904e-159</td>\n",
       "      <td>2.4300737494983045e-131</td>\n",
       "      <td>4.312732896470867e-106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022190742703687424</td>\n",
       "      <td>0.02345911784677907</td>\n",
       "      <td>0.026702213365201927</td>\n",
       "      <td>0.018698767194779</td>\n",
       "      <td>0.01612335234325684</td>\n",
       "      <td>0.02044533134739693</td>\n",
       "      <td>0.021637896723685157</td>\n",
       "      <td>0.02263003674157023</td>\n",
       "      <td>0.027307441410596628</td>\n",
       "      <td>0.02011791429020566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CrBH64C36N8Cl</td>\n",
       "      <td>2</td>\n",
       "      <td>7012071</td>\n",
       "      <td>aP</td>\n",
       "      <td>2.5539568684619016e-124</td>\n",
       "      <td>6.695090303446718e-100</td>\n",
       "      <td>3.388120188237275e-78</td>\n",
       "      <td>3.3099441410526027e-59</td>\n",
       "      <td>6.242262239106458e-43</td>\n",
       "      <td>2.2725991638009368e-29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010953850133145496</td>\n",
       "      <td>0.010919959753545946</td>\n",
       "      <td>0.006817093533223525</td>\n",
       "      <td>0.008086716249913872</td>\n",
       "      <td>0.009701348346822984</td>\n",
       "      <td>0.009343126204603031</td>\n",
       "      <td>0.007334721281056535</td>\n",
       "      <td>0.0062561204741455705</td>\n",
       "      <td>0.008446701357729658</td>\n",
       "      <td>0.0036373146821231757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Eu2FeH24(C10O11)2</td>\n",
       "      <td>2</td>\n",
       "      <td>7008637</td>\n",
       "      <td>aP</td>\n",
       "      <td>5.686240833143902e-253</td>\n",
       "      <td>1.7384807717941228e-217</td>\n",
       "      <td>1.0260629868378293e-184</td>\n",
       "      <td>1.1690622572250651e-154</td>\n",
       "      <td>2.5713473422725404e-127</td>\n",
       "      <td>1.0918006290735117e-102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02867391436636088</td>\n",
       "      <td>0.031033803078047005</td>\n",
       "      <td>0.039057301998247564</td>\n",
       "      <td>0.03898704388860709</td>\n",
       "      <td>0.02507342093700117</td>\n",
       "      <td>0.03382186635663629</td>\n",
       "      <td>0.03045758542496152</td>\n",
       "      <td>0.029351036893752944</td>\n",
       "      <td>0.02673088180210378</td>\n",
       "      <td>0.009560489640962186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>H14C12N3ClO</td>\n",
       "      <td>2</td>\n",
       "      <td>4028410</td>\n",
       "      <td>aP</td>\n",
       "      <td>9.045317999122451e-259</td>\n",
       "      <td>1.0703600344197437e-222</td>\n",
       "      <td>2.44531283634749e-189</td>\n",
       "      <td>1.0785819398103966e-158</td>\n",
       "      <td>9.185609731590592e-131</td>\n",
       "      <td>1.5105311861440632e-105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0058879323351490225</td>\n",
       "      <td>0.007809202343655805</td>\n",
       "      <td>0.008530364744848801</td>\n",
       "      <td>0.011710218456068544</td>\n",
       "      <td>0.013191126167859384</td>\n",
       "      <td>0.008141125582203772</td>\n",
       "      <td>0.007544759909944889</td>\n",
       "      <td>0.011308112774060576</td>\n",
       "      <td>0.008236930554108141</td>\n",
       "      <td>0.0016775081755523722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>H8C10BrNO2</td>\n",
       "      <td>2</td>\n",
       "      <td>2101508</td>\n",
       "      <td>aP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.640598962993535e-300</td>\n",
       "      <td>3.1131748648720986e-261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047728551371501074</td>\n",
       "      <td>0.03120793740024109</td>\n",
       "      <td>0.042555831837052326</td>\n",
       "      <td>0.06476668447641283</td>\n",
       "      <td>0.06995359625169179</td>\n",
       "      <td>0.0904456536047323</td>\n",
       "      <td>0.04204934287674289</td>\n",
       "      <td>0.03843552091578364</td>\n",
       "      <td>0.025166423778947004</td>\n",
       "      <td>0.008007778484757054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ba2H38.6C28S2O27</td>\n",
       "      <td>2</td>\n",
       "      <td>7207581</td>\n",
       "      <td>aP</td>\n",
       "      <td>3.1396655305950556e-215</td>\n",
       "      <td>1.903032664796368e-182</td>\n",
       "      <td>2.229150167577769e-152</td>\n",
       "      <td>5.046614617008923e-125</td>\n",
       "      <td>2.208349974610127e-100</td>\n",
       "      <td>1.8680278849380114e-78</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014810786653480854</td>\n",
       "      <td>0.01792716104257475</td>\n",
       "      <td>0.014655611969379846</td>\n",
       "      <td>0.00732172765073157</td>\n",
       "      <td>0.014516676653980674</td>\n",
       "      <td>0.01153828171294352</td>\n",
       "      <td>0.010333137458668049</td>\n",
       "      <td>0.009042307369004913</td>\n",
       "      <td>0.01295224715444796</td>\n",
       "      <td>0.00897608179695729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 184 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0   1        2   3                        4    \\\n",
       "1               H26C25N4ClO3F   2  2227539  aP   4.909847048222291e-137   \n",
       "2                  CoP3H36C23   2  4085709  aP  3.3916256736702194e-189   \n",
       "3              H34C34S2(N2O)3   2  2210818  aP   6.144994743224586e-218   \n",
       "4   P2H44Ru2C56N14Cl4O9.25F12   2  4312692  aP    4.146075234670375e-83   \n",
       "5                 H22C23SN2O3   2  2208064  aP  1.7606859769070014e-259   \n",
       "6               CrBH64C36N8Cl   2  7012071  aP  2.5539568684619016e-124   \n",
       "7           Eu2FeH24(C10O11)2   2  7008637  aP   5.686240833143902e-253   \n",
       "8                 H14C12N3ClO   2  4028410  aP   9.045317999122451e-259   \n",
       "9                  H8C10BrNO2   2  2101508  aP                      0.0   \n",
       "10           Ba2H38.6C28S2O27   2  7207581  aP  3.1396655305950556e-215   \n",
       "\n",
       "                        5                        6                        7    \\\n",
       "1    2.993258003847605e-111    3.522733828136009e-88    8.003408706033912e-68   \n",
       "2   1.2351214695284974e-158   8.683022289037302e-131  1.1783970561129091e-105   \n",
       "3    5.954486105173294e-185   1.113849578233596e-154    4.02224287840651e-127   \n",
       "4    1.3978668895159648e-63    9.098167871772325e-47    1.143145632965874e-32   \n",
       "5    2.249971724353988e-223   5.550494842694082e-190   2.643296218774904e-159   \n",
       "6    6.695090303446718e-100    3.388120188237275e-78   3.3099441410526027e-59   \n",
       "7   1.7384807717941228e-217  1.0260629868378293e-184  1.1690622572250651e-154   \n",
       "8   1.0703600344197437e-222    2.44531283634749e-189  1.0785819398103966e-158   \n",
       "9                       0.0                      0.0                      0.0   \n",
       "10   1.903032664796368e-182   2.229150167577769e-152   5.046614617008923e-125   \n",
       "\n",
       "                        8                        9    ...  \\\n",
       "1     3.510182114016871e-50    2.971965685119351e-35  ...   \n",
       "2     3.087250502676185e-83   1.5613906675494182e-63  ...   \n",
       "3    2.803945229831649e-102    3.773377444235225e-80  ...   \n",
       "4     2.772736921587006e-21   1.2983005990880624e-12  ...   \n",
       "5   2.4300737494983045e-131   4.312732896470867e-106  ...   \n",
       "6     6.242262239106458e-43   2.2725991638009368e-29  ...   \n",
       "7   2.5713473422725404e-127  1.0918006290735117e-102  ...   \n",
       "8    9.185609731590592e-131  1.5105311861440632e-105  ...   \n",
       "9    3.640598962993535e-300  3.1131748648720986e-261  ...   \n",
       "10   2.208349974610127e-100   1.8680278849380114e-78  ...   \n",
       "\n",
       "                      174                   175                   176  \\\n",
       "1    0.023087572208346268   0.03470161046711315  0.026240115052367056   \n",
       "2    0.013113304862799295   0.01394778016454681   0.01540255338368422   \n",
       "3    0.015220210645994606  0.012784898045865972  0.009240284757519177   \n",
       "4    0.006725776040262346  0.009491708355898012  0.011158401538297549   \n",
       "5    0.022190742703687424   0.02345911784677907  0.026702213365201927   \n",
       "6    0.010953850133145496  0.010919959753545946  0.006817093533223525   \n",
       "7     0.02867391436636088  0.031033803078047005  0.039057301998247564   \n",
       "8   0.0058879323351490225  0.007809202343655805  0.008530364744848801   \n",
       "9    0.047728551371501074   0.03120793740024109  0.042555831837052326   \n",
       "10   0.014810786653480854   0.01792716104257475  0.014655611969379846   \n",
       "\n",
       "                     177                   178                   179  \\\n",
       "1   0.018112677152580716  0.013301584321419636   0.01943209584729744   \n",
       "2    0.01378661526772984   0.02263673724886634  0.021858884972270273   \n",
       "3   0.013426332226241806  0.016861790459496533   0.02535214160778556   \n",
       "4   0.009486048490128369  0.009029170189152664    0.0137523993119171   \n",
       "5      0.018698767194779   0.01612335234325684   0.02044533134739693   \n",
       "6   0.008086716249913872  0.009701348346822984  0.009343126204603031   \n",
       "7    0.03898704388860709   0.02507342093700117   0.03382186635663629   \n",
       "8   0.011710218456068544  0.013191126167859384  0.008141125582203772   \n",
       "9    0.06476668447641283   0.06995359625169179    0.0904456536047323   \n",
       "10   0.00732172765073157  0.014516676653980674   0.01153828171294352   \n",
       "\n",
       "                     180                    181                    182  \\\n",
       "1    0.03018274253498323    0.01792259535015317   0.022856899978009926   \n",
       "2   0.020877693697405397   0.028194846009904313    0.02273571935202447   \n",
       "3   0.009457101150023196   0.008136538693339681  0.0067923851135896175   \n",
       "4     0.0116070571917301   0.009644470200327366  0.0073570053747198765   \n",
       "5   0.021637896723685157    0.02263003674157023   0.027307441410596628   \n",
       "6   0.007334721281056535  0.0062561204741455705   0.008446701357729658   \n",
       "7    0.03045758542496152   0.029351036893752944    0.02673088180210378   \n",
       "8   0.007544759909944889   0.011308112774060576   0.008236930554108141   \n",
       "9    0.04204934287674289    0.03843552091578364   0.025166423778947004   \n",
       "10  0.010333137458668049   0.009042307369004913    0.01295224715444796   \n",
       "\n",
       "                      183  \n",
       "1    0.006056703830570913  \n",
       "2    0.014079356216551837  \n",
       "3    0.004124100972046004  \n",
       "4   0.0037940031355739533  \n",
       "5     0.02011791429020566  \n",
       "6   0.0036373146821231757  \n",
       "7    0.009560489640962186  \n",
       "8   0.0016775081755523722  \n",
       "9    0.008007778484757054  \n",
       "10    0.00897608179695729  \n",
       "\n",
       "[10 rows x 184 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data loading and remove unnecessary column index\n",
    "data=pd.read_csv('C:/Users/anshi/Desktop/nano281/lab3/data/cod_xrd_42k.csv',header=None)\n",
    "Test=pd.read_csv('C:/Users/anshi/Desktop/nano281/lab3/data/test.csv',header=None)\n",
    "data1=data.iloc[1:]\n",
    "data1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X is XRD spectrum data Y is label for crystal structure\n",
    "X=data1.iloc[:,4:]\n",
    "Y=data1.iloc[:,3]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model training: Before we start to train the model, we first need to have a look at the crystal structure distribution in this data set. It is found that 14 types of crystal structures all presented in this dataset. Different types of crystal structure have different sample size. For instance, 'aP' type crystal has 4000 samples while 'oF' structure only have 406 samples. In order to avoid sampling imbalancing. We first categorize data into 14 groups and select 400 samples randomly in each group. So we have 400*14 = 5600 samples as sub-dataset to train our model. 100*14=1400 samples were selected for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 184)\n",
      "(3613, 184)\n",
      "(1242, 184)\n",
      "(2161, 184)\n",
      "(4000, 184)\n",
      "(4000, 184)\n",
      "(4000, 184)\n",
      "(4000, 184)\n",
      "(406, 184)\n",
      "(889, 184)\n",
      "(4000, 184)\n",
      "(2496, 184)\n",
      "(3337, 184)\n",
      "(3670, 184)\n"
     ]
    }
   ],
   "source": [
    "data_grouped = dict(tuple(data1.groupby(3)))\n",
    "list_data_grouped = [data_grouped[x] for x in data_grouped]\n",
    "print(list_data_grouped[0].shape)\n",
    "print(list_data_grouped[1].shape)\n",
    "print(list_data_grouped[2].shape)\n",
    "print(list_data_grouped[3].shape)\n",
    "print(list_data_grouped[4].shape)\n",
    "print(list_data_grouped[5].shape)\n",
    "print(list_data_grouped[6].shape)\n",
    "print(list_data_grouped[7].shape)\n",
    "print(list_data_grouped[8].shape)\n",
    "print(list_data_grouped[9].shape)\n",
    "print(list_data_grouped[10].shape)\n",
    "print(list_data_grouped[11].shape)\n",
    "print(list_data_grouped[12].shape)\n",
    "print(list_data_grouped[13].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "aP_train=list_data_grouped[0].sample(n = 400)\n",
    "aP_test=aP_train.sample(n = 100)\n",
    "\n",
    "cF_train=list_data_grouped[1].sample(n = 400)\n",
    "cF_test=cF_train.sample(n = 100)\n",
    "\n",
    "cI_train=list_data_grouped[2].sample(n = 400)\n",
    "cI_test=cI_train.sample(n = 100)\n",
    "\n",
    "cP_train=list_data_grouped[3].sample(n = 400)\n",
    "cP_test=cP_train.sample(n = 100)\n",
    "\n",
    "hP_train=list_data_grouped[4].sample(n = 400)\n",
    "hP_test=hP_train.sample(n = 100)\n",
    "\n",
    "hR_train=list_data_grouped[5].sample(n = 400)\n",
    "hR_test=hR_train.sample(n = 100)\n",
    "\n",
    "mP_train=list_data_grouped[6].sample(n = 400)\n",
    "mP_test=mP_train.sample(n = 100)\n",
    "\n",
    "mS_train=list_data_grouped[7].sample(n = 400)\n",
    "mS_test=mS_train.sample(n = 100)\n",
    "\n",
    "oF_train=list_data_grouped[8].sample(n = 400)\n",
    "oF_test=oF_train.sample(n = 100)\n",
    "\n",
    "oI_train=list_data_grouped[9].sample(n = 400)\n",
    "oI_test=oI_train.sample(n = 100)\n",
    "\n",
    "oP_train=list_data_grouped[10].sample(n = 400)\n",
    "oP_test=oP_train.sample(n = 100)\n",
    "\n",
    "oS_train=list_data_grouped[11].sample(n = 400)\n",
    "oS_test=oS_train.sample(n = 100)\n",
    "\n",
    "tI_train=list_data_grouped[12].sample(n = 400)\n",
    "tI_test=tI_train.sample(n = 100)\n",
    "\n",
    "tP_train=list_data_grouped[13].sample(n = 400)\n",
    "tP_test=tP_train.sample(n = 100)\n",
    "\n",
    "train=aP_train.append(cF_train).append(cI_train).append(cP_train).append(hP_train).append(hR_train).append(mP_train).append(mS_train).append(oF_train).append(oI_train).append(oP_train).append(oS_train).append(tI_train).append(tP_train)\n",
    "test=aP_test.append(cF_test).append(cI_test).append(cP_test).append(hP_test).append(hR_test).append(mP_test).append(mS_test).append(oF_test).append(oI_test).append(oP_test).append(oS_test).append(tI_test).append(tP_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training feature and label dimension==>> (5600, 180) (5600,)\n",
      "Test feature and label dimension==>> (1400, 180) (1400,)\n"
     ]
    }
   ],
   "source": [
    "X_train=train.iloc[:,4:]\n",
    "X_test=test.iloc[:,4:]\n",
    "Y_train=train.iloc[:,3]\n",
    "Y_test=test.iloc[:,3]\n",
    "feature = [x*0.5 for x in range(1,181)]\n",
    "feature=np.array(feature, dtype=float) \n",
    "feature.shape\n",
    "print('Training feature and label dimension==>>',X_train.shape, Y_train.shape)\n",
    "print('Test feature and label dimension==>>',X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear based models are first employed to perform classification due to their quickly running time. \n",
    "\n",
    "LogistisRegression and SVM model were introduced. GridSearchCV is imported to tune all the selected parameters.\n",
    "\n",
    "LogisticRegression: Tuned Parameter {'penalty': ['l1','l2'], 'C': [0.01,0.1,1,10],'solver':['newton-cg', 'lbfgs','liblinear','sag']}\n",
    "Best performance parameter:{'C': 1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
    "Model performance for our sub-dataset:\n",
    "Training Accuracy : 0.32160714285714287\n",
    "Validation Accuracy : 0.31357142857142856"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "nan (+/-nan) for {'C': 0.01, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "nan (+/-nan) for {'C': 0.01, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "0.005 (+/-0.000) for {'C': 0.01, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "nan (+/-nan) for {'C': 0.01, 'penalty': 'l1', 'solver': 'sag'}\n",
      "0.196 (+/-0.011) for {'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.196 (+/-0.011) for {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.182 (+/-0.019) for {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.196 (+/-0.011) for {'C': 0.01, 'penalty': 'l2', 'solver': 'sag'}\n",
      "nan (+/-nan) for {'C': 0.1, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "nan (+/-nan) for {'C': 0.1, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "0.155 (+/-0.081) for {'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "nan (+/-nan) for {'C': 0.1, 'penalty': 'l1', 'solver': 'sag'}\n",
      "0.202 (+/-0.011) for {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.202 (+/-0.011) for {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.197 (+/-0.008) for {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.202 (+/-0.011) for {'C': 0.1, 'penalty': 'l2', 'solver': 'sag'}\n",
      "nan (+/-nan) for {'C': 1, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "nan (+/-nan) for {'C': 1, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "0.200 (+/-0.016) for {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "nan (+/-nan) for {'C': 1, 'penalty': 'l1', 'solver': 'sag'}\n",
      "0.212 (+/-0.025) for {'C': 1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.213 (+/-0.025) for {'C': 1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.205 (+/-0.012) for {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.212 (+/-0.025) for {'C': 1, 'penalty': 'l2', 'solver': 'sag'}\n",
      "nan (+/-nan) for {'C': 10, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "nan (+/-nan) for {'C': 10, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "0.205 (+/-0.025) for {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "nan (+/-nan) for {'C': 10, 'penalty': 'l1', 'solver': 'sag'}\n",
      "0.209 (+/-0.031) for {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.207 (+/-0.029) for {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.202 (+/-0.020) for {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.209 (+/-0.031) for {'C': 10, 'penalty': 'l2', 'solver': 'sag'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          aP       0.45      0.56      0.50       400\n",
      "          cF       0.38      0.65      0.48       400\n",
      "          cI       0.30      0.38      0.33       400\n",
      "          cP       0.30      0.36      0.33       400\n",
      "          hP       0.34      0.30      0.32       400\n",
      "          hR       0.24      0.21      0.22       400\n",
      "          mP       0.34      0.36      0.35       400\n",
      "          mS       0.29      0.21      0.24       400\n",
      "          oF       0.33      0.35      0.34       400\n",
      "          oI       0.26      0.19      0.22       400\n",
      "          oP       0.29      0.28      0.29       400\n",
      "          oS       0.31      0.23      0.26       400\n",
      "          tI       0.27      0.18      0.22       400\n",
      "          tP       0.28      0.23      0.25       400\n",
      "\n",
      "    accuracy                           0.32      5600\n",
      "   macro avg       0.31      0.32      0.31      5600\n",
      "weighted avg       0.31      0.32      0.31      5600\n",
      "\n",
      "Confusion matrix, without normalization\n",
      "[[225   0   0   6   0  13  70  27   5   9  34   4   1   6]\n",
      " [  0 260  45  32  15   1   0   0   7   3   1   3  20  13]\n",
      " [  1  95 153  42  17  17   0   1  21  15   2   4  17  15]\n",
      " [  5  74  49 146  19  16   0   6  10  11   6  20  14  24]\n",
      " [  6  50  54  38 122  23   6   8  21  13   7  18  15  19]\n",
      " [ 15  51  25  39  28  84  11  21  29  19  16  20  18  24]\n",
      " [ 99   1   4   3   1  13 144  40  17   8  57   5   3   5]\n",
      " [ 52   8   6  14   7  21  63  85  28  22  56  19   7  12]\n",
      " [  2  22  24  26  27  28   6  20 141  24  23  30  16  11]\n",
      " [  9  32  36  27  37  26  15  13  29  75  19  18  29  35]\n",
      " [ 52   3   4  11  12  20  70  27  26  14 112  24   7  18]\n",
      " [ 15  16  27  27  25  35  15  25  40  23  19  91  18  24]\n",
      " [  7  53  55  37  29  25   6  11  28  28   6  20  72  23]\n",
      " [ 15  28  34  36  23  28  15  12  19  24  23  21  31  91]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-deb4b18cbebe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     clf = GridSearchCV(LogisticRegression(random_state=0), tuned_parameters, cv=5,\n\u001b[0;32m     12\u001b[0m                        scoring='%s_macro' % score)\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best parameters set found on development set:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    706\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 708\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    709\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1032\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1033\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    845\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    529\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1405\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1406\u001b[0m             \u001b[0mprefer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'processes'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[0;32m   1408\u001b[0m                                \u001b[1;33m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprefer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m             path_func(X, y, pos_class=class_, Cs=[C_],\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    845\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[0;32m    795\u001b[0m                 \u001b[0mbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0ml1_ratio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 797\u001b[1;33m             w0, n_iter_i, warm_start_sag = sag_solver(\n\u001b[0m\u001b[0;32m    798\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m                 \u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py\u001b[0m in \u001b[0;36msag_solver\u001b[1;34m(X, y, sample_weight, loss, alpha, beta, max_iter, tol, verbose, random_state, check_input, max_squared_sum, warm_start_mem, is_saga)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[0msag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msag64\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msag32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m     num_seen, n_iter_ = sag(dataset, coef_init,\n\u001b[0m\u001b[0;32m    312\u001b[0m                             \u001b[0mintercept_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m                             \u001b[0mn_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#LogisticRegression Parameter Tuning\n",
    "tuned_parameters = [{'penalty': ['l1','l2'], 'C': [0.01,0.1,1,10],'solver':['newton-cg', 'lbfgs','liblinear','sag']}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "classes = ['aP' 'cF' 'cI' 'cP' 'hP' 'hR' 'mP' 'mS' 'oF' 'oI' 'oP' 'oS' 'tI' 'tP']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(LogisticRegression(random_state=0), tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    \n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    \n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    # evaluate model on same training set\n",
    "    Y_true, Y_pred = Y_train, clf.predict(X_train)\n",
    "    print(classification_report(Y_true, Y_pred))\n",
    "    \n",
    "    cm = confusion_matrix(Y_true, Y_pred)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm, classes=classes,# normalize=True\n",
    "                          title='CM of model estimation on training data')\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        3 pre\n",
      "1020   aP  cP\n",
      "1467   aP  mP\n",
      "1496   aP  aP\n",
      "827    aP  mS\n",
      "3019   aP  mP\n",
      "...    ..  ..\n",
      "38727  tP  aP\n",
      "39167  tP  aP\n",
      "41387  tP  tI\n",
      "41363  tP  oP\n",
      "41200  tP  oS\n",
      "\n",
      "[5600 rows x 2 columns]\n",
      "Training Accuracy : 0.32160714285714287\n",
      "Validation Accuracy : 0.31357142857142856\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression Evaluation\n",
    "clf = LogisticRegression(C=1, penalty='l2', solver= 'lbfgs',random_state=0)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "# evaluate model on same training set\n",
    "Y_true, Y_pred = Y_train, clf.predict(X_train)\n",
    "Y_true = pd.DataFrame(Y_train)\n",
    "Y_pred = pd.DataFrame(data=Y_pred)\n",
    "YPre=Y_pred[0].to_list()\n",
    "Y_true['pre']=YPre\n",
    "print(Y_true)\n",
    "print(\"Training Accuracy :\", clf.score(X_train, Y_train))\n",
    "print(\"Validation Accuracy :\", clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM: Tuned Parameter:  \n",
    "[{'kernel': ['rbf', 'linear', 'sigmoid'], 'gamma': [1e-3, 1e-4],'C': [1, 10, 20]}]\n",
    "Best performance parameter: {'C': 20, 'gamma': 0.001, 'kernel': 'linear'}\n",
    "Model performance for our sub-dataset:\n",
    "Training Accuracy : 0.4419642857142857\n",
    "Validation Accuracy : 0.4257142857142857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM Parameter Tuning\n",
    "tuned_parameters = [\n",
    "    {'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],'C': [1, 10, 20]},\n",
    "    {'kernel': ['linear'],'gamma': [1e-3, 1e-4], 'C': [1, 10, 20]},\n",
    "    {'kernel': ['sigmoid'],'gamma': [1e-3, 1e-4], 'C': [1, 10, 20]}\n",
    "    ]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "classes = ['aP' 'cF' 'cI' 'cP' 'hP' 'hR' 'mP' 'mS' 'oF' 'oI' 'oP' 'oS' 'tI' 'tP']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    \n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    \n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    \n",
    "    # evaluate model on same training set\n",
    "    Y_true, Y_pred = Y_train, clf.predict(X_train)\n",
    "    \n",
    "    print(classification_report(Y_true, Y_pred))\n",
    "    cm = confusion_matrix(Y_true, Y_pred)\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm, classes=classes,# normalize=True\n",
    "                          title='CM of model estimation on training data')\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        3 pre\n",
      "1020   aP  aP\n",
      "1467   aP  mP\n",
      "1496   aP  aP\n",
      "827    aP  aP\n",
      "3019   aP  mP\n",
      "...    ..  ..\n",
      "38727  tP  aP\n",
      "39167  tP  aP\n",
      "41387  tP  tI\n",
      "41363  tP  oP\n",
      "41200  tP  oS\n",
      "\n",
      "[5600 rows x 2 columns]\n",
      "Training Accuracy : 0.4419642857142857\n",
      "Validation Accuracy : 0.4257142857142857\n"
     ]
    }
   ],
   "source": [
    "#SVM Evaluation\n",
    "clf = SVC(C=20, gamma=0.001,kernel='linear')\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "# evaluate model on same training set\n",
    "Y_true, Y_pred = Y_train, clf.predict(X_train)\n",
    "Y_true = pd.DataFrame(Y_train)\n",
    "Y_pred = pd.DataFrame(data=Y_pred)\n",
    "YPre=Y_pred[0].to_list()\n",
    "Y_true['pre']=YPre\n",
    "print(Y_true)\n",
    "print(\"Training Accuracy :\", clf.score(X_train, Y_train))\n",
    "print(\"Validation Accuracy :\", clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All the GridSearchCV parameters tuning are the same procedure. So I will not paste all the repeating content. Instead, I will show the training results for our best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Since we took 180 XRD data points from 0.5 degree to 90 degree as features. All the signals are obtained due to materials intrinsic properties. So it is hard to say there are certain linear relationship between 2theta degree and crystal structures. Thus, linear model can't satisfy our demanding of classification task. That's why we got low accuracy using linear model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### However, Cluster ensembles seem to be a good solution to categories 14 crystal structure based on spectrum similarity. Thus, we selected two ensemble model to train our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### RandomForest: Tuned Parameters:\n",
    "{'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [100,200,300,400],'max_depth': [20, 40, 60, 80, 100, None]}\n",
    "Best performance: {'max_depth': 30, 'max_features': 'sqrt', 'n_estimators': 400}\n",
    "Model performance for our sub-dataset:\n",
    "Training Accuracy : 1.0\n",
    "Validation Accuracy : 1.0\n",
    "It looks perfect for RandomForest with 1.0 accuracy for both training and validation. However, this extremely high accuracy might be ascribed to overfitting in such a small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        3 pre\n",
      "1020   aP  aP\n",
      "1467   aP  aP\n",
      "1496   aP  aP\n",
      "827    aP  aP\n",
      "3019   aP  aP\n",
      "...    ..  ..\n",
      "38727  tP  tP\n",
      "39167  tP  tP\n",
      "41387  tP  tP\n",
      "41363  tP  tP\n",
      "41200  tP  tP\n",
      "\n",
      "[5600 rows x 2 columns]\n",
      "Training Accuracy : 1.0\n",
      "Validation Accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "#RandomForest\n",
    "clf = RandomForestClassifier(oob_score = True,bootstrap=True, max_depth=30, max_features='sqrt',n_estimators=400)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "# evaluate model on same training set\n",
    "Y_true, Y_pred = Y_train, clf.predict(X_train)\n",
    "Y_true = pd.DataFrame(Y_train)\n",
    "Y_pred = pd.DataFrame(data=Y_pred)\n",
    "YPre=Y_pred[0].to_list()\n",
    "Y_true['pre']=YPre\n",
    "print(Y_true)\n",
    "print(\"Training Accuracy :\", clf.score(X_train, Y_train))\n",
    "print(\"Validation Accuracy :\", clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adaboost: Tuned Parameters:\n",
    "{'n_estimators': [100,200,300],'learning_rate':[0.1,1,10]}\n",
    "Best Performance parameters: {'learning_rate': 0.1, 'n_estimators': 100}\n",
    "Model performance for our sub-dataset:\n",
    "Training Accuracy : 0.22089285714285714\n",
    "Validation Accuracy : 0.23      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        3 pre\n",
      "1020   aP  mS\n",
      "1467   aP  mS\n",
      "1496   aP  mP\n",
      "827    aP  mP\n",
      "3019   aP  mP\n",
      "...    ..  ..\n",
      "38727  tP  mP\n",
      "39167  tP  oS\n",
      "41387  tP  tI\n",
      "41363  tP  mP\n",
      "41200  tP  tI\n",
      "\n",
      "[5600 rows x 2 columns]\n",
      "Training Accuracy : 0.22089285714285714\n",
      "Validation Accuracy : 0.23\n"
     ]
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(n_estimators=100, learning_rate=0.1)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "# evaluate model on same training set\n",
    "Y_true, Y_pred = Y_train, clf.predict(X_train)\n",
    "Y_true = pd.DataFrame(Y_train)\n",
    "Y_pred = pd.DataFrame(data=Y_pred)\n",
    "YPre=Y_pred[0].to_list()\n",
    "Y_true['pre']=YPre\n",
    "print(Y_true)\n",
    "print(\"Training Accuracy :\", clf.score(X_train, Y_train))\n",
    "print(\"Validation Accuracy :\", clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Deep learning method was also emoployed to study crystal classification. Multi-Layer Perceptron model and CNN model was carefully trained.\n",
    "Tuned Parameters:\n",
    "'hidden_layer_sizes':{(128,128,128), (256,256,256), (512,512,512), (512,512), (512,512,512,512)} --- Best: (512,512,512)\n",
    "'activation': {'identity', 'logistic', 'tanh', 'relu'} --- Best: 'relu'\n",
    "'solver':{'lbfgs', 'sgd', 'adam'} ---Best: 'sgd'\n",
    "'learning_rate_init':{[0.001, 0.01, 0.1, 1, 10]} --- Best: 0.1\n",
    "'max_iter': {[100, 200, 300, 400]} --- Best: 100\n",
    "'alpha': {[0.01, 0.05, 0.1]} --- Best: 0.01\n",
    "Best parameters summary: {'hidden_layer_sizes':(512,512,512), 'learning_rate_init':0.1, 'random_state':1, 'alpha':0.01, 'max_iter':100, 'solver':'sgd', 'activation':'relu'}\n",
    "Model performance for our sub-dataset:\n",
    "Training Accuracy : 0.6032142857142857\n",
    "Validation Accuracy : 0.6035714285714285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        3 pre\n",
      "1020   aP  aP\n",
      "1467   aP  aP\n",
      "1496   aP  aP\n",
      "827    aP  aP\n",
      "3019   aP  aP\n",
      "...    ..  ..\n",
      "38727  tP  tP\n",
      "39167  tP  tP\n",
      "41387  tP  tP\n",
      "41363  tP  tP\n",
      "41200  tP  tP\n",
      "\n",
      "[5600 rows x 2 columns]\n",
      "Training Accuracy : 0.9933928571428572\n",
      "Validation Accuracy : 0.9892857142857143\n"
     ]
    }
   ],
   "source": [
    "#MLP\n",
    "clf = MLPClassifier(hidden_layer_sizes=(512,512,512),learning_rate_init=0.1,random_state=1, alpha=0.01, max_iter=100,solver='sgd',activation='relu')\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "# evaluate model on same training set\n",
    "Y_true, Y_pred = Y_train, clf.predict(X_train)\n",
    "Y_true = pd.DataFrame(Y_train)\n",
    "Y_pred = pd.DataFrame(data=Y_pred)\n",
    "YPre=Y_pred[0].to_list()\n",
    "Y_true['pre']=YPre\n",
    "print(Y_true)\n",
    "print(\"Training Accuracy :\", clf.score(X_train, Y_train))\n",
    "print(\"Validation Accuracy :\", clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label classes ==>> ['aP' 'cF' 'cI' 'cP' 'hP' 'hR' 'mP' 'mS' 'oF' 'oI' 'oP' 'oS' 'tI' 'tP']\n"
     ]
    }
   ],
   "source": [
    "#CNN\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout, Activation, Dense, Flatten\n",
    "from tensorflow.keras.layers import Convolution1D,AveragePooling1D,MaxPooling1D, Reshape, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "data=pd.read_csv('C:/Users/anshi/Desktop/nano281/lab3/data/cod_xrd_42k.csv',header=None)\n",
    "Test=pd.read_csv('C:/Users/anshi/Desktop/nano281/lab3/data/test.csv',header=None)\n",
    "data1=data.iloc[1:]\n",
    "Test1=Test.iloc[1:]\n",
    "data2=data1.sample(5000)\n",
    "feature=data2.iloc[:,4:]\n",
    "label=data2.iloc[:,3]\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "feature = min_max_scaler.fit_transform(feature)\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(label)\n",
    "print('label classes ==>>',le.classes_)\n",
    "label = le.transform(label)\n",
    "x1_train, x1_validation, Y_train, Y_validation = train_test_split(feature, label, test_size=0.2, random_state=42)\n",
    "Y_train = to_categorical(Y_train, 14)\n",
    "Y_validation = to_categorical(Y_validation, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 180, 14)           56000     \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 178, 6)            258       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 89, 6)             0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 89, 6)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 87, 16)            304       \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 85, 16)            784       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 42, 16)            0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 42, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 40, 32)            1568      \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 38, 32)            3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 19, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 19, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 17, 64)            6208      \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 15, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 5, 128)            24704     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 5, 120)            15480     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 5, 80)             9680      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 5, 14)             1134      \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 5, 14)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 5, 14)             210       \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_5 ( (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "loss (Activation)            (None, 14)                0         \n",
      "=================================================================\n",
      "Total params: 131,786\n",
      "Trainable params: 131,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "CNN Model created.\n"
     ]
    }
   ],
   "source": [
    "#CNN model followed by VGG16 with one-dimensional version\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=4000,output_dim=14,input_length=180))\n",
    "\n",
    "model.add(Convolution1D(filters=6,kernel_size=3,padding='valid',activation='relu',strides=1))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Convolution1D(filters=16,kernel_size=3,padding='valid',activation='relu',strides=1))\n",
    "model.add(Convolution1D(filters=16,kernel_size=3,padding='valid',activation='relu',strides=1))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Convolution1D(filters=32,kernel_size=3,padding='valid',activation='relu',strides=1))\n",
    "model.add(Convolution1D(filters=32,kernel_size=3,padding='valid',activation='relu',strides=1))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Convolution1D(filters=64,kernel_size=3,padding='valid',activation='relu',strides=1))\n",
    "model.add(Convolution1D(filters=64,kernel_size=3,padding='valid',activation='relu',strides=1))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "          \n",
    "model.add(Convolution1D(filters=128,kernel_size=3,padding='valid',activation='relu',strides=1))\n",
    "\n",
    "model.add(Dense(units=120,activation=\"relu\"))\n",
    "model.add(Dense(units=80,activation=\"relu\"))\n",
    "model.add(Dense(units=14, activation=\"softmax\"))\n",
    "\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Convolution1D(14, 1))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "model.add(Activation('softmax', name='loss'))\n",
    "opt = Adam(learning_rate=0.01)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "print(\"CNN Model created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 2.5501 - accuracy: 0.0993 - val_loss: 2.5431 - val_accuracy: 0.0740\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 2.5273 - accuracy: 0.0948 - val_loss: 2.5400 - val_accuracy: 0.0740\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 2s 17ms/step - loss: 2.5280 - accuracy: 0.0945 - val_loss: 2.5447 - val_accuracy: 0.0740\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 2.5297 - accuracy: 0.0945 - val_loss: 2.5427 - val_accuracy: 0.0740\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 2.5270 - accuracy: 0.0965 - val_loss: 2.5439 - val_accuracy: 0.0740\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 2.5260 - accuracy: 0.0980 - val_loss: 2.5476 - val_accuracy: 0.0740\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 2.5259 - accuracy: 0.0990 - val_loss: 2.5416 - val_accuracy: 0.1000\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 2.5256 - accuracy: 0.0955 - val_loss: 2.5409 - val_accuracy: 0.1000\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 2.5273 - accuracy: 0.0955 - val_loss: 2.5412 - val_accuracy: 0.0740\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 2.5261 - accuracy: 0.1050 - val_loss: 2.5496 - val_accuracy: 0.0740\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 2.5270 - accuracy: 0.0962 - val_loss: 2.5467 - val_accuracy: 0.0740\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 2.5265 - accuracy: 0.1000 - val_loss: 2.5415 - val_accuracy: 0.1060\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 2.5264 - accuracy: 0.0925 - val_loss: 2.5468 - val_accuracy: 0.0740\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 2.5267 - accuracy: 0.1007 - val_loss: 2.5450 - val_accuracy: 0.0740\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 1s 12ms/step - loss: 2.5269 - accuracy: 0.0915 - val_loss: 2.5408 - val_accuracy: 0.0740\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 2.5257 - accuracy: 0.0965 - val_loss: 2.5453 - val_accuracy: 0.0740\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 2.5266 - accuracy: 0.1007 - val_loss: 2.5438 - val_accuracy: 0.0740\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 2.5252 - accuracy: 0.0927 - val_loss: 2.5418 - val_accuracy: 0.0740\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 2.5265 - accuracy: 0.0983 - val_loss: 2.5410 - val_accuracy: 0.0740\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 2.5260 - accuracy: 0.1055 - val_loss: 2.5424 - val_accuracy: 0.1000\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 2.5262 - accuracy: 0.0965 - val_loss: 2.5434 - val_accuracy: 0.1000\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 2.5244 - accuracy: 0.0988 - val_loss: 2.5446 - val_accuracy: 0.0740\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 2.5262 - accuracy: 0.0960 - val_loss: 2.5451 - val_accuracy: 0.0740\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 2.5271 - accuracy: 0.0978 - val_loss: 2.5434 - val_accuracy: 0.0940\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 2.5261 - accuracy: 0.1025 - val_loss: 2.5448 - val_accuracy: 0.0940\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 2.5252 - accuracy: 0.1050 - val_loss: 2.5416 - val_accuracy: 0.1000\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 2.5258 - accuracy: 0.1018 - val_loss: 2.5486 - val_accuracy: 0.0740\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 2.5263 - accuracy: 0.1002 - val_loss: 2.5408 - val_accuracy: 0.0740\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 2.5265 - accuracy: 0.0920 - val_loss: 2.5429 - val_accuracy: 0.0740\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 1s 12ms/step - loss: 2.5250 - accuracy: 0.1047 - val_loss: 2.5405 - val_accuracy: 0.0740\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 2.5248 - accuracy: 0.1030 - val_loss: 2.5422 - val_accuracy: 0.0940\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 2.5265 - accuracy: 0.0967 - val_loss: 2.5400 - val_accuracy: 0.0940\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 2.5251 - accuracy: 0.0967 - val_loss: 2.5448 - val_accuracy: 0.0940\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 2.5248 - accuracy: 0.1007 - val_loss: 2.5446 - val_accuracy: 0.0740\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 2.5243 - accuracy: 0.1075 - val_loss: 2.5482 - val_accuracy: 0.0740\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 1s 12ms/step - loss: 2.5267 - accuracy: 0.0920 - val_loss: 2.5439 - val_accuracy: 0.0740\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 2.5250 - accuracy: 0.1035 - val_loss: 2.5437 - val_accuracy: 0.0740\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 2.5248 - accuracy: 0.0927 - val_loss: 2.5456 - val_accuracy: 0.0740\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 1s 12ms/step - loss: 2.5259 - accuracy: 0.1032 - val_loss: 2.5395 - val_accuracy: 0.0940\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 2.5259 - accuracy: 0.0932 - val_loss: 2.5423 - val_accuracy: 0.0740\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 2.5261 - accuracy: 0.0945 - val_loss: 2.5430 - val_accuracy: 0.0940\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 1s 12ms/step - loss: 2.5254 - accuracy: 0.1030 - val_loss: 2.5424 - val_accuracy: 0.1000\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 1s 12ms/step - loss: 2.5259 - accuracy: 0.0962 - val_loss: 2.5416 - val_accuracy: 0.0740\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 2.5264 - accuracy: 0.1020 - val_loss: 2.5396 - val_accuracy: 0.0940\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - 1s 10ms/step - loss: 2.5264 - accuracy: 0.0983 - val_loss: 2.5419 - val_accuracy: 0.1000\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 2.5254 - accuracy: 0.0997 - val_loss: 2.5427 - val_accuracy: 0.0740\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 2.5257 - accuracy: 0.0953 - val_loss: 2.5442 - val_accuracy: 0.0740\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 2.5249 - accuracy: 0.1055 - val_loss: 2.5417 - val_accuracy: 0.1000\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 2.5259 - accuracy: 0.1018 - val_loss: 2.5455 - val_accuracy: 0.0740\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 2.5245 - accuracy: 0.1050 - val_loss: 2.5401 - val_accuracy: 0.0940\n"
     ]
    }
   ],
   "source": [
    "history  = model.fit(x1_train,Y_train, epochs = 50, batch_size = 32, validation_data = (x1_validation,Y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### MLP-based deep learning model exhibits superb performance while CNN model (VGG16-like architecture) shows extremely low accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We could clearly notice that only RandomForest and MLP model have relatively high accuracy. Rest of models have poor performance. We then apply these optimized model to whole dataset. All the results are shown below. It is predictable that RandomForest and MLP have better performance than other models. RandomForest has highest validation accuracy with 0.536 score. In order to improve our model performance, we involve class-volting algorithm to merge RandomForest and MLP models. However, the class-volting accuracy decreased, which might ascribed to different classification mechanisms behind these two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Model evaluation for overall dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### In the above section, we use small dataset to train and optimize all types of model using GridSearchCV. Next, we apply all the prepared model to take prediction of overall dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.9997858290201678\n",
      "Validation Accuracy : 0.5446046814986594\n"
     ]
    }
   ],
   "source": [
    "#RandomForest\n",
    "clf1 = RandomForestClassifier(bootstrap=True,min_samples_leaf=1, max_depth=30, max_features='sqrt',min_samples_split=4, n_estimators=600)\n",
    "clf1.fit(x_train, y_train)\n",
    "print(\"Training Accuracy :\", clf1.score(x_train, y_train))\n",
    "print(\"Validation Accuracy :\", clf1.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.9927181866857041\n",
      "Validation Accuracy : 0.487354156098268\n"
     ]
    }
   ],
   "source": [
    "#ExtraTree\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "cl=ExtraTreesClassifier(n_estimators=500, max_depth=30, max_features=10, n_jobs=-1, warm_start=False)\n",
    "cl.fit(x_train, y_train)\n",
    "print(\"Training Accuracy :\", cl.score(x_train, y_train))\n",
    "print(\"Validation Accuracy :\", cl.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.9075852222023916\n",
      "Validation Accuracy : 0.4251032683527792\n"
     ]
    }
   ],
   "source": [
    "#MLP\n",
    "clf2 = MLPClassifier(hidden_layer_sizes=(512,512,512), learning_rate_init=0.1, random_state=1, alpha=0.01, max_iter=100, solver='sgd', activation='relu')\n",
    "clf2.fit(x_train, y_train)\n",
    "print(\"Training Accuracy :\", clf2.score(x_train, y_train))\n",
    "print(\"Validation Accuracy :\", clf2.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.2847403176869534\n",
      "Validation Accuracy : 0.2825567070077542\n"
     ]
    }
   ],
   "source": [
    "#AdaBoost\n",
    "clf3 = AdaBoostClassifier(n_estimators=100, learning_rate=0.1)\n",
    "clf3.fit(x_train, y_train)\n",
    "print(\"Training Accuracy :\", clf3.score(x_train, y_train))\n",
    "print(\"Validation Accuracy :\", clf3.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.33485632696769585\n",
      "Validation Accuracy : 0.3077034567722299\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "clf4 = SVC(C=20, gamma=0.001, kernel='linear')\n",
    "clf4.fit(x_train, y_train)\n",
    "print(\"Training Accuracy :\", clf4.score(x_train, y_train))\n",
    "print(\"Validation Accuracy :\", clf4.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.3130108870248081\n",
      "Validation Accuracy : 0.29639828973113996\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression\n",
    "clf5 = LogisticRegression(C=1, penalty='l2', solver= 'lbfgs',random_state=0)\n",
    "clf5.fit(x_train, y_train)\n",
    "print(\"Training Accuracy :\", clf5.score(x_train, y_train))\n",
    "print(\"Validation Accuracy :\", clf5.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.33485632696769585\n",
      "Validation Accuracy : 0.3077034567722299\n"
     ]
    }
   ],
   "source": [
    "#Volting FOR RandomForest and MLP\n",
    "clf6 = VotingClassifier(estimators=[('rf', clf1), ('mlp', clf2)])\n",
    "clf6.fit(x_train, y_train)\n",
    "print(\"Training Accuracy :\", clf4.score(x_train, y_train))\n",
    "print(\"Validation Accuracy :\", clf4.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.5801534892022131\n",
      "Validation Accuracy : 0.41850858757881004\n"
     ]
    }
   ],
   "source": [
    "#GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier(n_estimators=100)\n",
    "model.fit(x_train, y_train)\n",
    "print(\"Training Accuracy :\", model.score(x_train, y_train))\n",
    "print(\"Validation Accuracy :\", model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 1.0\n",
      "Validation Accuracy : 0.37770852960359447\n"
     ]
    }
   ],
   "source": [
    "#DecisionTree\n",
    "from sklearn import tree\n",
    "model = tree.DecisionTreeClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "print(\"Training Accuracy :\", model.score(x_train, y_train))\n",
    "print(\"Validation Accuracy :\", model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### In general, Ensembling method exhibit best performance comparatively. The overall ranking based on accuracy:\n",
    "RandomForest > ExtraTree > MLP > GradientBoosting > DecisionTree > SVM > AdaBoost > CNN > LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Best 3 model after trained and optimized:\n",
    "Model1: RandomForestClassifier(oob_score = True,bootstrap=True, max_depth=30, max_features='sqrt',n_estimators=600)\n",
    "Model2: ExtraTreesClassifier(n_estimators=500, max_depth=30, max_features=10, n_jobs=-1, warm_start=False)\n",
    "Model3: MLPClassifier(hidden_layer_sizes=(512,512,512), learning_rate_init=0.1, random_state=1, alpha=0.01, max_iter=100, solver='sgd', activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 Feature Pruning for model improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To further improve our model performance, my intention is to decrease unuseful/unimportant/noise-like features to achive feature pruning. This method is widely used to decrease overfitting issues. My first trial is to keep top 20 highest significant data point and leave others to set as 0. For example, if we set N=30, only top 30 numerical data will be saved and 5/6 data discarded (set as 0. We study the accuracy for RandomForest model and MLP model with 1/6, 2/6, 3/6, 4/6, 5/6 kept data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy for RF: 0.9968231304658218\n",
      "Validation Accuracy for RF : 0.5482281324733677\n",
      "Training Accuracy for MLP: 0.9302159557379975\n",
      "Validation Accuracy for MLP: 0.4815566345387347\n"
     ]
    }
   ],
   "source": [
    "N = 30\n",
    "X2 = X.mask(X.rank(axis=1, method='min', ascending=False) > N, 0)\n",
    "X2_train, X2_test, Y_train, Y_test = train_test_split(X2, Y, test_size=0.33, random_state=42)\n",
    "clf1.fit(X2_train, Y_train)\n",
    "print(\"Training Accuracy for RF:\", clf1.score(X2_train, Y_train))\n",
    "print(\"Validation Accuracy for RF :\", clf1.score(X2_test, Y_test))\n",
    "clf2.fit(X2_train, Y_train)\n",
    "print(\"Training Accuracy for MLP:\", clf2.score(X2_train, Y_train))\n",
    "print(\"Validation Accuracy for MLP:\", clf2.score(X2_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy for RF: 1.0\n",
      "Validation Accuracy for RF : 0.5519965214870642\n",
      "Training Accuracy for MLP: 0.8913082277351418\n",
      "Validation Accuracy for MLP: 0.4383650989202116\n"
     ]
    }
   ],
   "source": [
    "N = 60\n",
    "X2 = X.mask(X.rank(axis=1, method='min', ascending=False) > N, 0)\n",
    "X2_train, X2_test, Y_train, Y_test = train_test_split(X2, Y, test_size=0.33, random_state=42)\n",
    "clf1.fit(X2_train, Y_train)\n",
    "print(\"Training Accuracy for RF:\", clf1.score(X2_train, Y_train))\n",
    "print(\"Validation Accuracy for RF :\", clf1.score(X2_test, Y_test))\n",
    "clf2.fit(X2_train, Y_train)\n",
    "print(\"Training Accuracy for MLP:\", clf2.score(X2_train, Y_train))\n",
    "print(\"Validation Accuracy for MLP:\", clf2.score(X2_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy for RF: 1.0\n",
      "Validation Accuracy for RF : 0.5493876367852742\n",
      "Training Accuracy for MLP: 0.9765482777083705\n",
      "Validation Accuracy for MLP: 0.48554243061091384\n"
     ]
    }
   ],
   "source": [
    "N = 90\n",
    "X2 = X.mask(X.rank(axis=1, method='min', ascending=False) > N, 0)\n",
    "X2_train, X2_test, Y_train, Y_test = train_test_split(X2, Y, test_size=0.33, random_state=42)\n",
    "clf1.fit(X2_train, Y_train)\n",
    "print(\"Training Accuracy for RF:\", clf1.score(X2_train, Y_train))\n",
    "print(\"Validation Accuracy for RF :\", clf1.score(X2_test, Y_test))\n",
    "clf2.fit(X2_train, Y_train)\n",
    "print(\"Training Accuracy for MLP:\", clf2.score(X2_train, Y_train))\n",
    "print(\"Validation Accuracy for MLP:\", clf2.score(X2_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy for RF: 1.0\n",
      "Validation Accuracy for RF : 0.55888107833901\n",
      "Training Accuracy for MLP: 0.9490273067999286\n",
      "Validation Accuracy for MLP: 0.4760489890571781\n"
     ]
    }
   ],
   "source": [
    "N = 120\n",
    "X2 = X.mask(X.rank(axis=1, method='min', ascending=False) > N, 0)\n",
    "X2_train, X2_test, Y_train, Y_test = train_test_split(X2, Y, test_size=0.33, random_state=42)\n",
    "clf1.fit(X2_train, Y_train)\n",
    "print(\"Training Accuracy for RF:\", clf1.score(X2_train, Y_train))\n",
    "print(\"Validation Accuracy for RF :\", clf1.score(X2_test, Y_test))\n",
    "clf2.fit(X2_train, Y_train)\n",
    "print(\"Training Accuracy for MLP:\", clf2.score(X2_train, Y_train))\n",
    "print(\"Validation Accuracy for MLP:\", clf2.score(X2_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy for RF: 1.0\n",
      "Validation Accuracy for RF : 0.57714327125154\n",
      "Training Accuracy for MLP: 0.9453150098161699\n",
      "Validation Accuracy for MLP: 0.4489455757663599\n"
     ]
    }
   ],
   "source": [
    "N = 150\n",
    "X2 = X.mask(X.rank(axis=1, method='min', ascending=False) > N, 0)\n",
    "X2_train, X2_test, Y_train, Y_test = train_test_split(X2, Y, test_size=0.33, random_state=42)\n",
    "clf1.fit(X2_train, Y_train)\n",
    "print(\"Training Accuracy for RF:\", clf1.score(X2_train, Y_train))\n",
    "print(\"Validation Accuracy for RF :\", clf1.score(X2_test, Y_test))\n",
    "clf2.fit(X2_train, Y_train)\n",
    "print(\"Training Accuracy for MLP:\", clf2.score(X2_train, Y_train))\n",
    "print(\"Validation Accuracy for MLP:\", clf2.score(X2_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy for RF: 1.0\n",
      "Validation Accuracy for RF : 0.5382998768026669\n",
      "Training Accuracy for MLP: 0.9075852222023916\n",
      "Validation Accuracy for MLP: 0.4251032683527792\n"
     ]
    }
   ],
   "source": [
    "N = 180\n",
    "X2 = X.mask(X.rank(axis=1, method='min', ascending=False) > N, 0)\n",
    "X2_train, X2_test, Y_train, Y_test = train_test_split(X2, Y, test_size=0.33, random_state=42)\n",
    "clf1.fit(X2_train, Y_train)\n",
    "print(\"Training Accuracy for RF:\", clf1.score(X2_train, Y_train))\n",
    "print(\"Validation Accuracy for RF :\", clf1.score(X2_test, Y_test))\n",
    "clf2.fit(X2_train, Y_train)\n",
    "print(\"Training Accuracy for MLP:\", clf2.score(X2_train, Y_train))\n",
    "print(\"Validation Accuracy for MLP:\", clf2.score(X2_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### It is found that RandomForest model reach to highest accuracy when we are keeping 5/6 features while MLP model tends to be best if we leave 3/6 features for training. Later, we keep feature numbers to be 5/6 and export prediction results for my submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test1=Test.iloc[1:]\n",
    "Label=Test[0]\n",
    "Test1=Test1.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4379</th>\n",
       "      <td>aP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4380</th>\n",
       "      <td>mS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4381</th>\n",
       "      <td>oP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4382</th>\n",
       "      <td>cF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4383</th>\n",
       "      <td>tP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4384 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0     oS\n",
       "1     hR\n",
       "2     cF\n",
       "3     hP\n",
       "4     cF\n",
       "...   ..\n",
       "4379  aP\n",
       "4380  mS\n",
       "4381  oP\n",
       "4382  cF\n",
       "4383  tP\n",
       "\n",
       "[4384 rows x 1 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 150\n",
    "clf= RandomForestClassifier(oob_score = True,bootstrap=True, max_depth=90, max_features='sqrt',n_estimators=600)\n",
    "X2 = X.mask(X.rank(axis=1, method='min', ascending=False) > N, 0)\n",
    "Test2 = Test1.mask(Test1.rank(axis=1, method='min', ascending=False) > N, 0)\n",
    "clf.fit(X2_train, Y_train)\n",
    "\n",
    "Test_pred=clf.predict(Test2)\n",
    "Test_pred=pd.DataFrame(data=Test_pred)\n",
    "Test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         id\n",
       "1          0\n",
       "2          1\n",
       "3          2\n",
       "4          3\n",
       "        ... \n",
       "4380    4379\n",
       "4381    4380\n",
       "4382    4381\n",
       "4383    4382\n",
       "4384    4383\n",
       "Name: 0, Length: 4385, dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_pred.to_csv(r'C:/Users/anshi/Desktop/nano281/lab3/data/predict3.csv', index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part4 Peak Position Extraction for feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### In section 3, we are trying to cut off features manually based on numerical value. Here, we hope to extract peak position and intensity for each spectrum. Then we could do further analysis based on peak position. So our training/test dataset will simplified to numerical numbers for peak intensity and 0 value for non-peak position. Four promising methods were employed to re-train and predict the data. However, it exhibits lower accuracy compared with overall dataset. Efforts made to improve the performance. We enlarge the peak-interval distance to increase the feature size for our samples. The accuracy increased slightly with larger gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>4375</th>\n",
       "      <th>4376</th>\n",
       "      <th>4377</th>\n",
       "      <th>4378</th>\n",
       "      <th>4379</th>\n",
       "      <th>4380</th>\n",
       "      <th>4381</th>\n",
       "      <th>4382</th>\n",
       "      <th>4383</th>\n",
       "      <th>4384</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.49427352490773e-193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.836053614469574e-272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.99901e-133</td>\n",
       "      <td>1.44137e-290</td>\n",
       "      <td>2.0867e-119</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.12954e-174</td>\n",
       "      <td>2.42499e-198</td>\n",
       "      <td>1.96223e-272</td>\n",
       "      <td>0</td>\n",
       "      <td>6.75813e-227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.309795102032758e-162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.625747029226458e-235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.09033e-108</td>\n",
       "      <td>2.10274e-252</td>\n",
       "      <td>1.32566e-95</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.73645e-145</td>\n",
       "      <td>4.20289e-167</td>\n",
       "      <td>1.31931e-235</td>\n",
       "      <td>0</td>\n",
       "      <td>2.27528e-193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2143493750591663e-133</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.394312835435316e-288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.79690941078256e-201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.14742e-86</td>\n",
       "      <td>5.92181e-217</td>\n",
       "      <td>1.6258e-74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.79796e-118</td>\n",
       "      <td>1.40619e-138</td>\n",
       "      <td>1.7124e-201</td>\n",
       "      <td>0</td>\n",
       "      <td>1.47877e-162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.894409283427328e-108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.105456733173771e-250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.086047846127684e-169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.36069e-66</td>\n",
       "      <td>3.21946e-184</td>\n",
       "      <td>3.84912e-56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.95344e-94</td>\n",
       "      <td>9.0824e-113</td>\n",
       "      <td>4.29065e-170</td>\n",
       "      <td>0</td>\n",
       "      <td>1.85536e-134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.411010035560565e-85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3340118112828542e-214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0774212337423548e-140</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.75003e-49</td>\n",
       "      <td>3.37887e-154</td>\n",
       "      <td>1.75919e-40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.6328e-73</td>\n",
       "      <td>1.13244e-89</td>\n",
       "      <td>2.07539e-141</td>\n",
       "      <td>0</td>\n",
       "      <td>4.49382e-109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.0424812634102474</td>\n",
       "      <td>0.00208803005959424</td>\n",
       "      <td>0.00015926859131056264</td>\n",
       "      <td>0.0012927062515306596</td>\n",
       "      <td>2.6605575490701784e-06</td>\n",
       "      <td>2.7667962226972557e-12</td>\n",
       "      <td>0.025596472400937337</td>\n",
       "      <td>0.017929374996234298</td>\n",
       "      <td>0.036719191095308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00460095</td>\n",
       "      <td>0.00685188</td>\n",
       "      <td>0.0167169</td>\n",
       "      <td>0.00251682</td>\n",
       "      <td>0.00238943</td>\n",
       "      <td>0.023967</td>\n",
       "      <td>0.040279</td>\n",
       "      <td>0.00802135</td>\n",
       "      <td>1.1909e-13</td>\n",
       "      <td>0.00976086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.014937943523705468</td>\n",
       "      <td>0.011913452627093807</td>\n",
       "      <td>0.04940593345492341</td>\n",
       "      <td>0.00025226436207982184</td>\n",
       "      <td>0.02021273702345255</td>\n",
       "      <td>4.265181765861382e-06</td>\n",
       "      <td>0.034317486765768145</td>\n",
       "      <td>0.001558421260420618</td>\n",
       "      <td>0.04672566154245738</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000415825</td>\n",
       "      <td>0.00981713</td>\n",
       "      <td>0.0127033</td>\n",
       "      <td>0.00574427</td>\n",
       "      <td>0.00155163</td>\n",
       "      <td>0.0263588</td>\n",
       "      <td>0.0394609</td>\n",
       "      <td>0.0046568</td>\n",
       "      <td>5.13572e-23</td>\n",
       "      <td>0.00970008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.03961150511969587</td>\n",
       "      <td>0.005427529038107739</td>\n",
       "      <td>0.02958608954985015</td>\n",
       "      <td>0.0005294737510236629</td>\n",
       "      <td>0.2964401894193597</td>\n",
       "      <td>0.015295729401235034</td>\n",
       "      <td>0.022283970166647567</td>\n",
       "      <td>1.0813241726351645e-06</td>\n",
       "      <td>0.061772980791212385</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0351359</td>\n",
       "      <td>0.0474156</td>\n",
       "      <td>0.0190289</td>\n",
       "      <td>2.53091e-05</td>\n",
       "      <td>1.94537e-06</td>\n",
       "      <td>0.034628</td>\n",
       "      <td>0.0371468</td>\n",
       "      <td>0.0130034</td>\n",
       "      <td>4.2755e-35</td>\n",
       "      <td>0.0207857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.02214655646888064</td>\n",
       "      <td>0.009212502824844869</td>\n",
       "      <td>3.420231589612979e-05</td>\n",
       "      <td>2.1807256184489826e-06</td>\n",
       "      <td>0.008392831937009959</td>\n",
       "      <td>0.1637547090637108</td>\n",
       "      <td>0.009691806568855649</td>\n",
       "      <td>0.002409214746824785</td>\n",
       "      <td>0.02086664367204997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0524932</td>\n",
       "      <td>0.0524935</td>\n",
       "      <td>0.00972469</td>\n",
       "      <td>2.15267e-10</td>\n",
       "      <td>4.7084e-12</td>\n",
       "      <td>0.0214137</td>\n",
       "      <td>0.0255503</td>\n",
       "      <td>0.0175345</td>\n",
       "      <td>6.87121e-50</td>\n",
       "      <td>0.0174716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.016810346633745706</td>\n",
       "      <td>0.004776666880218361</td>\n",
       "      <td>7.632783577108269e-11</td>\n",
       "      <td>1.7338723668423713e-11</td>\n",
       "      <td>4.5871132164784525e-07</td>\n",
       "      <td>0.0054449198421137645</td>\n",
       "      <td>0.0065715263708479715</td>\n",
       "      <td>0.014178631329935897</td>\n",
       "      <td>0.014036909761931438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0159239</td>\n",
       "      <td>0.0158505</td>\n",
       "      <td>0.00600167</td>\n",
       "      <td>3.53459e-18</td>\n",
       "      <td>2.19991e-20</td>\n",
       "      <td>0.01523</td>\n",
       "      <td>0.0158578</td>\n",
       "      <td>0.004589</td>\n",
       "      <td>2.13176e-67</td>\n",
       "      <td>0.000521693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows Ã— 4384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     1                        2                       3     \\\n",
       "1                     0.0    8.49427352490773e-193                     0.0   \n",
       "2                     0.0   7.309795102032758e-162                     0.0   \n",
       "3                     0.0  1.2143493750591663e-133                     0.0   \n",
       "4                     0.0   3.894409283427328e-108                     0.0   \n",
       "5                     0.0    2.411010035560565e-85                     0.0   \n",
       "..                    ...                      ...                     ...   \n",
       "176    0.0424812634102474      0.00208803005959424  0.00015926859131056264   \n",
       "177  0.014937943523705468     0.011913452627093807     0.04940593345492341   \n",
       "178   0.03961150511969587     0.005427529038107739     0.02958608954985015   \n",
       "179   0.02214655646888064     0.009212502824844869   3.420231589612979e-05   \n",
       "180  0.016810346633745706     0.004776666880218361   7.632783577108269e-11   \n",
       "\n",
       "                       4                       5                       6     \\\n",
       "1                       0.0                     0.0                     0.0   \n",
       "2                       0.0                     0.0                     0.0   \n",
       "3                       0.0                     0.0                     0.0   \n",
       "4                       0.0                     0.0                     0.0   \n",
       "5                       0.0                     0.0                     0.0   \n",
       "..                      ...                     ...                     ...   \n",
       "176   0.0012927062515306596  2.6605575490701784e-06  2.7667962226972557e-12   \n",
       "177  0.00025226436207982184     0.02021273702345255   4.265181765861382e-06   \n",
       "178   0.0005294737510236629      0.2964401894193597    0.015295729401235034   \n",
       "179  2.1807256184489826e-06    0.008392831937009959      0.1637547090637108   \n",
       "180  1.7338723668423713e-11  4.5871132164784525e-07   0.0054449198421137645   \n",
       "\n",
       "                        7                       8                        9     \\\n",
       "1                        0.0                     0.0   7.836053614469574e-272   \n",
       "2                        0.0                     0.0   5.625747029226458e-235   \n",
       "3     5.394312835435316e-288                     0.0    7.79690941078256e-201   \n",
       "4     6.105456733173771e-250                     0.0   2.086047846127684e-169   \n",
       "5    1.3340118112828542e-214                     0.0  1.0774212337423548e-140   \n",
       "..                       ...                     ...                      ...   \n",
       "176     0.025596472400937337    0.017929374996234298        0.036719191095308   \n",
       "177     0.034317486765768145    0.001558421260420618      0.04672566154245738   \n",
       "178     0.022283970166647567  1.0813241726351645e-06     0.061772980791212385   \n",
       "179     0.009691806568855649    0.002409214746824785      0.02086664367204997   \n",
       "180    0.0065715263708479715    0.014178631329935897     0.014036909761931438   \n",
       "\n",
       "    10    ...          4375          4376         4377         4378  \\\n",
       "1    0.0  ...  2.99901e-133  1.44137e-290  2.0867e-119            0   \n",
       "2    0.0  ...  3.09033e-108  2.10274e-252  1.32566e-95            0   \n",
       "3    0.0  ...   6.14742e-86  5.92181e-217   1.6258e-74            0   \n",
       "4    0.0  ...   2.36069e-66  3.21946e-184  3.84912e-56            0   \n",
       "5    0.0  ...   1.75003e-49  3.37887e-154  1.75919e-40            0   \n",
       "..   ...  ...           ...           ...          ...          ...   \n",
       "176  0.0  ...    0.00460095    0.00685188    0.0167169   0.00251682   \n",
       "177  0.0  ...   0.000415825    0.00981713    0.0127033   0.00574427   \n",
       "178  0.0  ...     0.0351359     0.0474156    0.0190289  2.53091e-05   \n",
       "179  0.0  ...     0.0524932     0.0524935   0.00972469  2.15267e-10   \n",
       "180  0.0  ...     0.0159239     0.0158505   0.00600167  3.53459e-18   \n",
       "\n",
       "            4379          4380          4381          4382         4383  \\\n",
       "1              0  4.12954e-174  2.42499e-198  1.96223e-272            0   \n",
       "2              0  7.73645e-145  4.20289e-167  1.31931e-235            0   \n",
       "3              0  2.79796e-118  1.40619e-138   1.7124e-201            0   \n",
       "4              0   1.95344e-94   9.0824e-113  4.29065e-170            0   \n",
       "5              0    2.6328e-73   1.13244e-89  2.07539e-141            0   \n",
       "..           ...           ...           ...           ...          ...   \n",
       "176   0.00238943      0.023967      0.040279    0.00802135   1.1909e-13   \n",
       "177   0.00155163     0.0263588     0.0394609     0.0046568  5.13572e-23   \n",
       "178  1.94537e-06      0.034628     0.0371468     0.0130034   4.2755e-35   \n",
       "179   4.7084e-12     0.0214137     0.0255503     0.0175345  6.87121e-50   \n",
       "180  2.19991e-20       0.01523     0.0158578      0.004589  2.13176e-67   \n",
       "\n",
       "             4384  \n",
       "1    6.75813e-227  \n",
       "2    2.27528e-193  \n",
       "3    1.47877e-162  \n",
       "4    1.85536e-134  \n",
       "5    4.49382e-109  \n",
       "..            ...  \n",
       "176    0.00976086  \n",
       "177    0.00970008  \n",
       "178     0.0207857  \n",
       "179     0.0174716  \n",
       "180   0.000521693  \n",
       "\n",
       "[180 rows x 4384 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test1.T\n",
    "df = pd.DataFrame() \n",
    "for i in range(0,4384):\n",
    "    \n",
    "    df[i]=Test1.iloc[i][(Test1.iloc[i].shift(1) < Test1.iloc[i]) & (Test1.iloc[i].shift(-1) < Test1.iloc[i])]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna()\n",
    "df=df.fillna(0)\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>6</th>\n",
       "      <th>10</th>\n",
       "      <th>13</th>\n",
       "      <th>16</th>\n",
       "      <th>23</th>\n",
       "      <th>27</th>\n",
       "      <th>29</th>\n",
       "      <th>31</th>\n",
       "      <th>36</th>\n",
       "      <th>41</th>\n",
       "      <th>...</th>\n",
       "      <th>153</th>\n",
       "      <th>155</th>\n",
       "      <th>158</th>\n",
       "      <th>162</th>\n",
       "      <th>164</th>\n",
       "      <th>167</th>\n",
       "      <th>170</th>\n",
       "      <th>172</th>\n",
       "      <th>175</th>\n",
       "      <th>178</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.1266235482471863e-296</td>\n",
       "      <td>3.445354931443829e-158</td>\n",
       "      <td>3.0053006878558532e-83</td>\n",
       "      <td>9.76061732257514e-33</td>\n",
       "      <td>7.045868475862041e-10</td>\n",
       "      <td>3.8161684036585716e-07</td>\n",
       "      <td>0.015677919466082296</td>\n",
       "      <td>0.008725200633657431</td>\n",
       "      <td>0.054364842674285714</td>\n",
       "      <td>0.15438137115569245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04097449927981326</td>\n",
       "      <td>0.08928982336316231</td>\n",
       "      <td>0.05890361097209178</td>\n",
       "      <td>0.0560837585288915</td>\n",
       "      <td>0.052548049289424636</td>\n",
       "      <td>0.05095331930208576</td>\n",
       "      <td>0.06019970169471312</td>\n",
       "      <td>0.05034175172667157</td>\n",
       "      <td>0.07802616196086258</td>\n",
       "      <td>0.03961150511969587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>4.225503550856159e-12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.811643575583189e-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00426075228427172</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011278245252938735</td>\n",
       "      <td>0.0172162995675801</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.01939206229857982</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.395288246438848e-227</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3.89115090116618e-68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.108738947006101e-36</td>\n",
       "      <td>0</td>\n",
       "      <td>9.911529016601049e-10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.06192395712461661</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.09277536340138914</td>\n",
       "      <td>0</td>\n",
       "      <td>3.957500825425765e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.288696420821052e-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.740605002344216e-49</td>\n",
       "      <td>6.760513387382762e-13</td>\n",
       "      <td>0.2964401894193597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4379</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.202536</td>\n",
       "      <td>0</td>\n",
       "      <td>0.413212</td>\n",
       "      <td>0.327806</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0322142</td>\n",
       "      <td>0</td>\n",
       "      <td>0.034628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4380</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0127141</td>\n",
       "      <td>0</td>\n",
       "      <td>0.264583</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.476279</td>\n",
       "      <td>0.169073</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0251863</td>\n",
       "      <td>0.0450462</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4381</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.164834</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0251733</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0254446</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4382</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.218137</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.107901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4383</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.532953</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.112861</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0297981</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0170342</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022428</td>\n",
       "      <td>0.0207857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4384 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          6                       10                      13   \\\n",
       "0     4.1266235482471863e-296  3.445354931443829e-158  3.0053006878558532e-83   \n",
       "1                           0   4.225503550856159e-12                     1.0   \n",
       "2                           0                       0                       0   \n",
       "3                           0                       0                       0   \n",
       "4                           0                       0                       0   \n",
       "...                       ...                     ...                     ...   \n",
       "4379                        0                       0                       0   \n",
       "4380                        0                       0               0.0127141   \n",
       "4381                        0                       0                       0   \n",
       "4382                        0                       0                       0   \n",
       "4383                        0                       0                       0   \n",
       "\n",
       "                        16                     23                      27   \\\n",
       "0      9.76061732257514e-33  7.045868475862041e-10  3.8161684036585716e-07   \n",
       "1     8.811643575583189e-14                      0                       0   \n",
       "2                         0                      0                       0   \n",
       "3                         0                      0                       0   \n",
       "4                         0                      0                       0   \n",
       "...                     ...                    ...                     ...   \n",
       "4379                      0               0.202536                       0   \n",
       "4380                      0               0.264583                       0   \n",
       "4381                      0                      0                       0   \n",
       "4382                      0                      0                       0   \n",
       "4383                      0                      0                       0   \n",
       "\n",
       "                       29                    31                    36   \\\n",
       "0     0.015677919466082296  0.008725200633657431  0.054364842674285714   \n",
       "1                        0   0.00426075228427172                     0   \n",
       "2                        0                     0                     0   \n",
       "3                        0                     0                     0   \n",
       "4                        0                     0                     0   \n",
       "...                    ...                   ...                   ...   \n",
       "4379              0.413212              0.327806                     0   \n",
       "4380                     0              0.476279              0.169073   \n",
       "4381              0.164834                     0                     0   \n",
       "4382                     0                     0                     0   \n",
       "4383              0.532953                     0                     0   \n",
       "\n",
       "                         41   ...                  153                   155  \\\n",
       "0        0.15438137115569245  ...  0.04097449927981326   0.08928982336316231   \n",
       "1                          0  ...                    0  0.011278245252938735   \n",
       "2     5.395288246438848e-227  ...                    0  3.89115090116618e-68   \n",
       "3                          0  ...                    0   0.06192395712461661   \n",
       "4                          0  ...                    0                     0   \n",
       "...                      ...  ...                  ...                   ...   \n",
       "4379                       0  ...                    0                     0   \n",
       "4380                       0  ...                    0                     0   \n",
       "4381                       0  ...                    0             0.0251733   \n",
       "4382                       0  ...                    0                     0   \n",
       "4383                0.112861  ...                    0                     0   \n",
       "\n",
       "                      158                    162                   164  \\\n",
       "0     0.05890361097209178     0.0560837585288915  0.052548049289424636   \n",
       "1      0.0172162995675801                      0                     0   \n",
       "2                       0                      0                     0   \n",
       "3                       0                      0   0.09277536340138914   \n",
       "4                       0  9.288696420821052e-14                     0   \n",
       "...                   ...                    ...                   ...   \n",
       "4379                    0                      0                     0   \n",
       "4380                    0                      0                     0   \n",
       "4381                    0                      0             0.0254446   \n",
       "4382                    0                      0              0.218137   \n",
       "4383            0.0297981                      0                     0   \n",
       "\n",
       "                      167                    170                    172  \\\n",
       "0     0.05095331930208576    0.06019970169471312    0.05034175172667157   \n",
       "1                       0    0.01939206229857982                      0   \n",
       "2                       0  6.108738947006101e-36                      0   \n",
       "3                       0  3.957500825425765e-07                      0   \n",
       "4                       0                      0  5.740605002344216e-49   \n",
       "...                   ...                    ...                    ...   \n",
       "4379                    0                      0              0.0322142   \n",
       "4380            0.0251863              0.0450462                      0   \n",
       "4381                    0                      0                      0   \n",
       "4382                    0                      0               0.107901   \n",
       "4383            0.0170342                      0                      0   \n",
       "\n",
       "                        175                  178  \n",
       "0       0.07802616196086258  0.03961150511969587  \n",
       "1                         0                    0  \n",
       "2     9.911529016601049e-10                    0  \n",
       "3                         0                    0  \n",
       "4     6.760513387382762e-13   0.2964401894193597  \n",
       "...                     ...                  ...  \n",
       "4379                      0             0.034628  \n",
       "4380                      0                    0  \n",
       "4381                      0                    0  \n",
       "4382                      0                    0  \n",
       "4383               0.022428            0.0207857  \n",
       "\n",
       "[4384 rows x 57 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=df.T\n",
    "df_copy = Test1.iloc[0:0,:].copy()\n",
    "for i in [ 6,  10,  13,  16,  23,  27,  29,  31,  36,  41,  43,  45,  49,\n",
    "             53,  55,  57,  59,  61,  65,  68,  70,  72,  74,  77,  79,  82,\n",
    "             84,  88,  90,  92,  97,  99, 101, 104, 107, 110, 115, 118, 122,\n",
    "            127, 131, 134, 136, 138, 144, 146, 149, 153, 155, 158, 162, 164,\n",
    "            167, 170, 172, 175, 178]:\n",
    "\n",
    "    df_copy[i]=test[i]\n",
    "    \n",
    "df_copy=df_copy.fillna(0)\n",
    "df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame() \n",
    "for i in range(0,41814):\n",
    "    \n",
    "    df1[i]=X.iloc[i][(X.iloc[i].shift(1) < X.iloc[i]) & (X.iloc[i].shift(-1) < X.iloc[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([  7,  10,  12,  14,  16,  18,  21,  24,  26,  30,  33,  36,  40,\n",
       "             44,  47,  50,  52,  55,  58,  60,  62,  67,  69,  72,  74,  77,\n",
       "             79,  85,  87,  90,  93,  95,  97, 102, 104, 106, 111, 115, 117,\n",
       "            119, 122, 126, 129, 131, 134, 136, 140, 142, 146, 149, 151, 154,\n",
       "            156, 159, 163, 166, 169, 172, 175, 180, 182],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.isna()\n",
    "df1=df1.fillna(0)\n",
    "df1.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>41804</th>\n",
       "      <th>41805</th>\n",
       "      <th>41806</th>\n",
       "      <th>41807</th>\n",
       "      <th>41808</th>\n",
       "      <th>41809</th>\n",
       "      <th>41810</th>\n",
       "      <th>41811</th>\n",
       "      <th>41812</th>\n",
       "      <th>41813</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.683022289037302e-131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.098167871772325e-47</td>\n",
       "      <td>5.550494842694082e-190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.44531283634749e-189</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.087250502676185e-83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.772736921587006e-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.242262239106458e-43</td>\n",
       "      <td>2.5713473422725404e-127</td>\n",
       "      <td>9.185609731590592e-131</td>\n",
       "      <td>3.640598962993535e-300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.873223845302584e-32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.772310121603259e-64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.658878315288052e-43</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9.956402542195653e-06</td>\n",
       "      <td>0.2491763505190967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.743147804877191e-107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.1316907317517951</td>\n",
       "      <td>1.2960744527867848e-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.3831423291850972e-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0070273792099574e-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0.04554458245119397</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011717295758677358</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02108511188620052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0793155</td>\n",
       "      <td>0.099469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142779</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.029616847439317924</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012230659139992358</td>\n",
       "      <td>0.050972026122596166</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.046028</td>\n",
       "      <td>0.0586543</td>\n",
       "      <td>0.0243194</td>\n",
       "      <td>0.0685931</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.01540255338368422</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011158401538297549</td>\n",
       "      <td>0.026702213365201927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.039057301998247564</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1153</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.02263673724886634</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009701348346822984</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013191126167859384</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014516676653980674</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0193196</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0368903</td>\n",
       "      <td>0.0209257</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.186246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.028194846009904313</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011308112774060576</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0522632</td>\n",
       "      <td>0.00937611</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0308004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows Ã— 41814 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      1                       0     2      \\\n",
       "6    8.683022289037302e-131                     NaN   NaN   \n",
       "8     3.087250502676185e-83                     NaN   NaN   \n",
       "11    2.873223845302584e-32                     NaN   NaN   \n",
       "14    9.956402542195653e-06      0.2491763505190967   NaN   \n",
       "16       0.1316907317517951  1.2960744527867848e-05   NaN   \n",
       "..                      ...                     ...   ...   \n",
       "170     0.04554458245119397                     NaN   NaN   \n",
       "173    0.029616847439317924                     NaN   NaN   \n",
       "176     0.01540255338368422                     NaN   NaN   \n",
       "178     0.02263673724886634                     NaN   NaN   \n",
       "181    0.028194846009904313                     NaN   NaN   \n",
       "\n",
       "                     3                       4                      5      \\\n",
       "6    9.098167871772325e-47  5.550494842694082e-190                    NaN   \n",
       "8    2.772736921587006e-21                     NaN  6.242262239106458e-43   \n",
       "11                     NaN   9.772310121603259e-64                    NaN   \n",
       "14                     NaN                     NaN                    NaN   \n",
       "16                     1.0  2.3831423291850972e-05                    NaN   \n",
       "..                     ...                     ...                    ...   \n",
       "170                    NaN                     NaN                    NaN   \n",
       "173                    NaN                     NaN                    NaN   \n",
       "176   0.011158401538297549    0.026702213365201927                    NaN   \n",
       "178                    NaN                     NaN   0.009701348346822984   \n",
       "181                    NaN                     NaN                    NaN   \n",
       "\n",
       "                       6                       7                       8      \\\n",
       "6                        NaN   2.44531283634749e-189                     NaN   \n",
       "8    2.5713473422725404e-127  9.185609731590592e-131  3.640598962993535e-300   \n",
       "11                       NaN                     NaN                     NaN   \n",
       "14                       NaN                     NaN  2.743147804877191e-107   \n",
       "16                       NaN  5.0070273792099574e-05                     NaN   \n",
       "..                       ...                     ...                     ...   \n",
       "170                      NaN    0.011717295758677358                     NaN   \n",
       "173                      NaN    0.012230659139992358    0.050972026122596166   \n",
       "176     0.039057301998247564                     NaN                     NaN   \n",
       "178                      NaN    0.013191126167859384                     NaN   \n",
       "181                      NaN    0.011308112774060576                     NaN   \n",
       "\n",
       "                     9      ...      41804      41805 41806      41807  \\\n",
       "6                      NaN  ...        NaN        NaN   NaN        NaN   \n",
       "8                      NaN  ...        NaN        NaN   NaN        NaN   \n",
       "11   9.658878315288052e-43  ...        NaN        NaN   NaN        NaN   \n",
       "14                     NaN  ...        NaN        NaN   NaN        NaN   \n",
       "16                     NaN  ...        NaN        NaN   NaN        NaN   \n",
       "..                     ...  ...        ...        ...   ...        ...   \n",
       "170    0.02108511188620052  ...  0.0793155   0.099469   NaN        NaN   \n",
       "173                    NaN  ...        NaN        NaN   NaN        NaN   \n",
       "176                    NaN  ...        NaN        NaN   NaN        NaN   \n",
       "178   0.014516676653980674  ...        NaN  0.0193196   NaN        NaN   \n",
       "181                    NaN  ...        NaN        NaN   NaN  0.0522632   \n",
       "\n",
       "          41808      41809      41810      41811     41812     41813  \n",
       "6           NaN        NaN        NaN        NaN       NaN       NaN  \n",
       "8           NaN        NaN        NaN        NaN       NaN       NaN  \n",
       "11          NaN        NaN        NaN        NaN       NaN       NaN  \n",
       "14          NaN        NaN        NaN        NaN       NaN       NaN  \n",
       "16          NaN        NaN        NaN        NaN       NaN       NaN  \n",
       "..          ...        ...        ...        ...       ...       ...  \n",
       "170         NaN        NaN        NaN        NaN  0.142779       NaN  \n",
       "173    0.046028  0.0586543  0.0243194  0.0685931       NaN       NaN  \n",
       "176         NaN        NaN        NaN     0.1153       NaN       NaN  \n",
       "178         NaN  0.0368903  0.0209257        NaN       NaN  0.186246  \n",
       "181  0.00937611        NaN        NaN  0.0308004       NaN       NaN  \n",
       "\n",
       "[61 rows x 41814 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1=df1.T\n",
    "df1_copy = X.iloc[0:0,:].copy()\n",
    "for i in [  7,  10,  12,  14,  16,  18,  21,  24,  26,  30,  33,  36,  40,\n",
    "             44,  47,  50,  52,  55,  58,  60,  62,  67,  69,  72,  74,  77,\n",
    "             79,  85,  87,  90,  93,  95,  97, 102, 104, 106, 111, 115, 117,\n",
    "            119, 122, 126, 129, 131, 134, 136, 140, 142, 146, 149, 151, 154,\n",
    "            156, 159, 163, 166, 169, 172, 175, 180, 182]:\n",
    "\n",
    "    df1_copy[i]=X1[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.683022289037302e-131</td>\n",
       "      <td>0</td>\n",
       "      <td>3.087250502676185e-83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.873223845302584e-32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.01540255338368422</td>\n",
       "      <td>0</td>\n",
       "      <td>0.02263673724886634</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.028194846009904313</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.098167871772325e-47</td>\n",
       "      <td>0</td>\n",
       "      <td>2.772736921587006e-21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011158401538297549</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.550494842694082e-190</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.772310121603259e-64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026702213365201927</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41810</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0368903</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41811</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0209257</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41812</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0308004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41813</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41814</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.186246</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41814 rows Ã— 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       1    2                       3    4                      5    6    7    \\\n",
       "1        0    0  8.683022289037302e-131    0  3.087250502676185e-83    0    0   \n",
       "2        0    0                       0    0                      0    0    0   \n",
       "3        0    0                       0    0                      0    0    0   \n",
       "4        0    0   9.098167871772325e-47    0  2.772736921587006e-21    0    0   \n",
       "5        0    0  5.550494842694082e-190    0                      0    0    0   \n",
       "...    ...  ...                     ...  ...                    ...  ...  ...   \n",
       "41810    0    0                       0    0                      0    0    0   \n",
       "41811    0    0                       0    0                      0    0    0   \n",
       "41812    0    0                       0    0                      0    0    0   \n",
       "41813    0    0                       0    0                      0    0    0   \n",
       "41814    0    0                       0    0                      0    0    0   \n",
       "\n",
       "                         8    9    10   ... 171  172                   173  \\\n",
       "1      2.873223845302584e-32    0    0  ...   0    0   0.01540255338368422   \n",
       "2                          0    0    0  ...   0    0                     0   \n",
       "3                          0    0    0  ...   0    0                     0   \n",
       "4                          0    0    0  ...   0    0  0.011158401538297549   \n",
       "5      9.772310121603259e-64    0    0  ...   0    0  0.026702213365201927   \n",
       "...                      ...  ...  ...  ...  ..  ...                   ...   \n",
       "41810                      0    0    0  ...   0    0                     0   \n",
       "41811                      0    0    0  ...   0    0                     0   \n",
       "41812                      0    0    0  ...   0    0                0.1153   \n",
       "41813                      0    0    0  ...   0    0                     0   \n",
       "41814                      0    0    0  ...   0    0                     0   \n",
       "\n",
       "       174                  175 176  177                   178  179 180  \n",
       "1        0  0.02263673724886634   0    0  0.028194846009904313    0   0  \n",
       "2        0                    0   0    0                     0    0   0  \n",
       "3        0                    0   0    0                     0    0   0  \n",
       "4        0                    0   0    0                     0    0   0  \n",
       "5        0                    0   0    0                     0    0   0  \n",
       "...    ...                  ...  ..  ...                   ...  ...  ..  \n",
       "41810    0            0.0368903   0    0                     0    0   0  \n",
       "41811    0            0.0209257   0    0                     0    0   0  \n",
       "41812    0                    0   0    0             0.0308004    0   0  \n",
       "41813    0                    0   0    0                     0    0   0  \n",
       "41814    0             0.186246   0    0                     0    0   0  \n",
       "\n",
       "[41814 rows x 180 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_copy=df1_copy.fillna(0)\n",
    "df1_copy=df1_copy.rename(columns=lambda s: s-3)\n",
    "df1_copy.index = np.arange(1, len(df1_copy) + 1)\n",
    "df1_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.1266235482471863e-296</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.445354931443829e-158</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.05034175172667157</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.07802616196086258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03961150511969587</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.225503550856159e-12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.911529016601049e-10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5.740605002344216e-49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.760513387382762e-13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2964401894193597</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4380</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0322142</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.034628</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4381</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4382</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4383</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.107901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4384</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022428</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0207857</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4384 rows Ã— 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1    2    3    4    5                        6    7    8    9    \\\n",
       "1       0    0    0    0    0  4.1266235482471863e-296    0    0    0   \n",
       "2       0    0    0    0    0                        0    0    0    0   \n",
       "3       0    0    0    0    0                        0    0    0    0   \n",
       "4       0    0    0    0    0                        0    0    0    0   \n",
       "5       0    0    0    0    0                        0    0    0    0   \n",
       "...   ...  ...  ...  ...  ...                      ...  ...  ...  ...   \n",
       "4380    0    0    0    0    0                        0    0    0    0   \n",
       "4381    0    0    0    0    0                        0    0    0    0   \n",
       "4382    0    0    0    0    0                        0    0    0    0   \n",
       "4383    0    0    0    0    0                        0    0    0    0   \n",
       "4384    0    0    0    0    0                        0    0    0    0   \n",
       "\n",
       "                         10   ...  171                    172 173  174  \\\n",
       "1     3.445354931443829e-158  ...    0    0.05034175172667157   0    0   \n",
       "2      4.225503550856159e-12  ...    0                      0   0    0   \n",
       "3                          0  ...    0                      0   0    0   \n",
       "4                          0  ...    0                      0   0    0   \n",
       "5                          0  ...    0  5.740605002344216e-49   0    0   \n",
       "...                      ...  ...  ...                    ...  ..  ...   \n",
       "4380                       0  ...    0              0.0322142   0    0   \n",
       "4381                       0  ...    0                      0   0    0   \n",
       "4382                       0  ...    0                      0   0    0   \n",
       "4383                       0  ...    0               0.107901   0    0   \n",
       "4384                       0  ...    0                      0   0    0   \n",
       "\n",
       "                        175 176  177                  178  179  180  \n",
       "1       0.07802616196086258   0    0  0.03961150511969587    0    0  \n",
       "2                         0   0    0                    0    0    0  \n",
       "3     9.911529016601049e-10   0    0                    0    0    0  \n",
       "4                         0   0    0                    0    0    0  \n",
       "5     6.760513387382762e-13   0    0   0.2964401894193597    0    0  \n",
       "...                     ...  ..  ...                  ...  ...  ...  \n",
       "4380                      0   0    0             0.034628    0    0  \n",
       "4381                      0   0    0                    0    0    0  \n",
       "4382                      0   0    0                    0    0    0  \n",
       "4383                      0   0    0                    0    0    0  \n",
       "4384               0.022428   0    0            0.0207857    0    0  \n",
       "\n",
       "[4384 rows x 180 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy.index = np.arange(1, len(df_copy) + 1)\n",
    "df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs=df1_copy\n",
    "xs_train, xs_test, y_train, y_test = train_test_split(Xs, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.9751204711761556\n",
      "Validation Accuracy : 0.3890136966446844\n"
     ]
    }
   ],
   "source": [
    "#RandomForest\n",
    "clf1 = RandomForestClassifier(oob_score = True,bootstrap=True, max_depth=30, n_estimators=600)\n",
    "clf1.fit(xs_train, y_train)\n",
    "print(\"Training Accuracy :\", clf1.score(xs_train, y_train))\n",
    "print(\"Validation Accuracy :\", clf1.score(xs_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.8903801534892022\n",
      "Validation Accuracy : 0.32082034930067393\n"
     ]
    }
   ],
   "source": [
    "#ExtraTree\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "cl=ExtraTreesClassifier(n_estimators=500, max_depth=30, max_features=10, n_jobs=-1, warm_start=False)\n",
    "cl.fit(xs_train, y_train)\n",
    "print(\"Training Accuracy :\", cl.score(xs_train, y_train))\n",
    "print(\"Validation Accuracy :\", cl.score(xs_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.8042834195966446\n",
      "Validation Accuracy : 0.2799478223059642\n"
     ]
    }
   ],
   "source": [
    "#MLP\n",
    "clf2 = MLPClassifier(hidden_layer_sizes=(512,512,512), learning_rate_init=0.1, random_state=1, alpha=0.01, max_iter=100, solver='sgd', activation='relu')\n",
    "clf2.fit(xs_train, y_train)\n",
    "print(\"Training Accuracy :\", clf2.score(xs_train, y_train))\n",
    "print(\"Validation Accuracy :\", clf2.score(xs_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.4458325896840978\n",
      "Validation Accuracy : 0.2990071744329299\n"
     ]
    }
   ],
   "source": [
    "#GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier(n_estimators=100)\n",
    "model.fit(xs_train, y_train)\n",
    "print(\"Training Accuracy :\", model.score(xs_train, y_train))\n",
    "print(\"Validation Accuracy :\", model.score(xs_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label classes ==>> ['aP' 'cF' 'cI' 'cP' 'hP' 'hR' 'mP' 'mS' 'oF' 'oI' 'oP' 'oS' 'tI' 'tP']\n"
     ]
    }
   ],
   "source": [
    "#CNN\n",
    "Xs['crystal']=Y\n",
    "Xs.sample(5000)\n",
    "feature1=Xs.iloc[:,:-1]\n",
    "label1=Xs.iloc[:,-1]\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "feature1 = min_max_scaler.fit_transform(feature1)\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(label1)\n",
    "print('label classes ==>>',le.classes_)\n",
    "label1 = le.transform(label1)\n",
    "x3_train, x3_validation, Y3_train, Y3_validation = train_test_split(feature1, label1, test_size=0.2, random_state=42)\n",
    "Y3_train = to_categorical(Y3_train, 14)\n",
    "Y3_validation = to_categorical(Y3_validation, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1046/1046 [==============================] - 13s 13ms/step - loss: 2.5295 - accuracy: 0.0963 - val_loss: 2.5309 - val_accuracy: 0.0936\n",
      "Epoch 2/50\n",
      "1046/1046 [==============================] - 15s 14ms/step - loss: 2.5297 - accuracy: 0.0920 - val_loss: 2.5314 - val_accuracy: 0.0928\n",
      "Epoch 3/50\n",
      "1046/1046 [==============================] - 15s 15ms/step - loss: 2.5302 - accuracy: 0.0955 - val_loss: 2.5293 - val_accuracy: 0.0977\n",
      "Epoch 4/50\n",
      "1046/1046 [==============================] - 15s 14ms/step - loss: 2.5297 - accuracy: 0.0944 - val_loss: 2.5298 - val_accuracy: 0.1014\n",
      "Epoch 5/50\n",
      "1046/1046 [==============================] - 14s 13ms/step - loss: 2.5296 - accuracy: 0.0958 - val_loss: 2.5311 - val_accuracy: 0.0977\n",
      "Epoch 6/50\n",
      "1046/1046 [==============================] - 14s 14ms/step - loss: 2.5293 - accuracy: 0.0976 - val_loss: 2.5300 - val_accuracy: 0.1014\n",
      "Epoch 7/50\n",
      "1046/1046 [==============================] - 15s 15ms/step - loss: 2.5300 - accuracy: 0.0939 - val_loss: 2.5334 - val_accuracy: 0.0928\n",
      "Epoch 8/50\n",
      "1046/1046 [==============================] - 18s 17ms/step - loss: 2.5293 - accuracy: 0.0960 - val_loss: 2.5327 - val_accuracy: 0.0977\n",
      "Epoch 9/50\n",
      "1046/1046 [==============================] - 18s 18ms/step - loss: 2.5299 - accuracy: 0.0944 - val_loss: 2.5323 - val_accuracy: 0.0929\n",
      "Epoch 10/50\n",
      "1046/1046 [==============================] - 18s 17ms/step - loss: 2.5295 - accuracy: 0.0963 - val_loss: 2.5295 - val_accuracy: 0.0977\n",
      "Epoch 11/50\n",
      "1046/1046 [==============================] - 18s 17ms/step - loss: 2.5293 - accuracy: 0.0943 - val_loss: 2.5298 - val_accuracy: 0.1014\n",
      "Epoch 12/50\n",
      "1046/1046 [==============================] - 18s 17ms/step - loss: 2.5299 - accuracy: 0.0941 - val_loss: 2.5295 - val_accuracy: 0.0928\n",
      "Epoch 13/50\n",
      "1046/1046 [==============================] - 17s 16ms/step - loss: 2.5297 - accuracy: 0.0948 - val_loss: 2.5317 - val_accuracy: 0.0928\n",
      "Epoch 14/50\n",
      "1046/1046 [==============================] - 17s 17ms/step - loss: 2.5301 - accuracy: 0.0936 - val_loss: 2.5303 - val_accuracy: 0.0929\n",
      "Epoch 15/50\n",
      "1046/1046 [==============================] - 18s 18ms/step - loss: 2.5296 - accuracy: 0.0949 - val_loss: 2.5331 - val_accuracy: 0.0936\n",
      "Epoch 16/50\n",
      "1046/1046 [==============================] - 16s 15ms/step - loss: 2.5294 - accuracy: 0.0965 - val_loss: 2.5293 - val_accuracy: 0.0977\n",
      "Epoch 17/50\n",
      "1046/1046 [==============================] - 18s 17ms/step - loss: 2.5297 - accuracy: 0.0950 - val_loss: 2.5300 - val_accuracy: 0.0929\n",
      "Epoch 18/50\n",
      "1046/1046 [==============================] - 18s 17ms/step - loss: 2.5296 - accuracy: 0.0956 - val_loss: 2.5310 - val_accuracy: 0.0929\n",
      "Epoch 19/50\n",
      "1046/1046 [==============================] - 17s 17ms/step - loss: 2.5296 - accuracy: 0.0970 - val_loss: 2.5316 - val_accuracy: 0.0929\n",
      "Epoch 20/50\n",
      "1046/1046 [==============================] - 17s 16ms/step - loss: 2.5295 - accuracy: 0.0961 - val_loss: 2.5328 - val_accuracy: 0.0936\n",
      "Epoch 21/50\n",
      "1046/1046 [==============================] - 17s 16ms/step - loss: 2.5299 - accuracy: 0.0963 - val_loss: 2.5308 - val_accuracy: 0.0936\n",
      "Epoch 22/50\n",
      "1046/1046 [==============================] - 16s 16ms/step - loss: 2.5291 - accuracy: 0.0970 - val_loss: 2.5308 - val_accuracy: 0.0928\n",
      "Epoch 23/50\n",
      "1046/1046 [==============================] - 20s 19ms/step - loss: 2.5295 - accuracy: 0.0949 - val_loss: 2.5303 - val_accuracy: 0.0936\n",
      "Epoch 24/50\n",
      "1046/1046 [==============================] - 18s 17ms/step - loss: 2.5297 - accuracy: 0.0964 - val_loss: 2.5295 - val_accuracy: 0.0928\n",
      "Epoch 25/50\n",
      "1046/1046 [==============================] - 18s 17ms/step - loss: 2.5295 - accuracy: 0.0948 - val_loss: 2.5296 - val_accuracy: 0.0878\n",
      "Epoch 26/50\n",
      "1046/1046 [==============================] - 18s 17ms/step - loss: 2.5298 - accuracy: 0.0955 - val_loss: 2.5311 - val_accuracy: 0.0929\n",
      "Epoch 27/50\n",
      "1046/1046 [==============================] - 19s 18ms/step - loss: 2.5298 - accuracy: 0.0939 - val_loss: 2.5291 - val_accuracy: 0.0977\n",
      "Epoch 28/50\n",
      "1046/1046 [==============================] - 21s 20ms/step - loss: 2.5296 - accuracy: 0.0956 - val_loss: 2.5295 - val_accuracy: 0.0929\n",
      "Epoch 29/50\n",
      "1046/1046 [==============================] - 17s 17ms/step - loss: 2.5297 - accuracy: 0.0958 - val_loss: 2.5318 - val_accuracy: 0.0928\n",
      "Epoch 30/50\n",
      "1046/1046 [==============================] - 19s 18ms/step - loss: 2.5296 - accuracy: 0.0978 - val_loss: 2.5307 - val_accuracy: 0.0936\n",
      "Epoch 31/50\n",
      "1046/1046 [==============================] - 19s 19ms/step - loss: 2.5296 - accuracy: 0.0960 - val_loss: 2.5318 - val_accuracy: 0.0928\n",
      "Epoch 32/50\n",
      "1046/1046 [==============================] - 18s 17ms/step - loss: 2.5300 - accuracy: 0.0917 - val_loss: 2.5296 - val_accuracy: 0.0977\n",
      "Epoch 33/50\n",
      "1046/1046 [==============================] - 19s 18ms/step - loss: 2.5297 - accuracy: 0.0938 - val_loss: 2.5325 - val_accuracy: 0.0928\n",
      "Epoch 34/50\n",
      "1046/1046 [==============================] - 18s 17ms/step - loss: 2.5296 - accuracy: 0.0959 - val_loss: 2.5315 - val_accuracy: 0.0936\n",
      "Epoch 35/50\n",
      "1046/1046 [==============================] - 17s 16ms/step - loss: 2.5298 - accuracy: 0.0916 - val_loss: 2.5342 - val_accuracy: 0.0977\n",
      "Epoch 36/50\n",
      "1046/1046 [==============================] - 17s 16ms/step - loss: 2.5296 - accuracy: 0.0946 - val_loss: 2.5328 - val_accuracy: 0.0929\n",
      "Epoch 37/50\n",
      "1046/1046 [==============================] - 20s 19ms/step - loss: 2.5298 - accuracy: 0.0962 - val_loss: 2.5298 - val_accuracy: 0.0929\n",
      "Epoch 38/50\n",
      "1046/1046 [==============================] - 18s 17ms/step - loss: 2.5294 - accuracy: 0.0977 - val_loss: 2.5301 - val_accuracy: 0.0936\n",
      "Epoch 39/50\n",
      "1046/1046 [==============================] - 20s 19ms/step - loss: 2.5300 - accuracy: 0.0935 - val_loss: 2.5293 - val_accuracy: 0.0928\n",
      "Epoch 40/50\n",
      "1046/1046 [==============================] - 18s 18ms/step - loss: 2.5297 - accuracy: 0.0964 - val_loss: 2.5303 - val_accuracy: 0.0929\n",
      "Epoch 41/50\n",
      "1046/1046 [==============================] - 19s 18ms/step - loss: 2.5291 - accuracy: 0.0980 - val_loss: 2.5309 - val_accuracy: 0.0929\n",
      "Epoch 42/50\n",
      "1046/1046 [==============================] - 19s 18ms/step - loss: 2.5298 - accuracy: 0.0951 - val_loss: 2.5299 - val_accuracy: 0.0936\n",
      "Epoch 43/50\n",
      "1046/1046 [==============================] - 19s 19ms/step - loss: 2.5292 - accuracy: 0.0928 - val_loss: 2.5375 - val_accuracy: 0.0929\n",
      "Epoch 44/50\n",
      "1046/1046 [==============================] - 19s 18ms/step - loss: 2.5294 - accuracy: 0.0943 - val_loss: 2.5301 - val_accuracy: 0.0936\n",
      "Epoch 45/50\n",
      "1046/1046 [==============================] - 17s 17ms/step - loss: 2.5297 - accuracy: 0.0966 - val_loss: 2.5295 - val_accuracy: 0.0977\n",
      "Epoch 46/50\n",
      "1046/1046 [==============================] - 15s 15ms/step - loss: 2.5293 - accuracy: 0.0950 - val_loss: 2.5319 - val_accuracy: 0.0878\n",
      "Epoch 47/50\n",
      "1046/1046 [==============================] - 17s 16ms/step - loss: 2.5294 - accuracy: 0.0955 - val_loss: 2.5316 - val_accuracy: 0.0928\n",
      "Epoch 48/50\n",
      "1046/1046 [==============================] - 18s 17ms/step - loss: 2.5297 - accuracy: 0.0930 - val_loss: 2.5302 - val_accuracy: 0.0929\n",
      "Epoch 49/50\n",
      "1046/1046 [==============================] - 19s 18ms/step - loss: 2.5295 - accuracy: 0.0932 - val_loss: 2.5358 - val_accuracy: 0.0929\n",
      "Epoch 50/50\n",
      "1046/1046 [==============================] - 19s 18ms/step - loss: 2.5297 - accuracy: 0.0936 - val_loss: 2.5309 - val_accuracy: 0.0929\n"
     ]
    }
   ],
   "source": [
    "history3  = model.fit(x3_train,Y3_train, epochs = 50, batch_size = 32, validation_data = (x3_validation,Y3_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.20278422273781901\n",
      "Validation Accuracy : 0.19733314008261468\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "clf4 = SVC(C=20, gamma=0.001, kernel='linear')\n",
    "clf4.fit(xs_train, y_train)\n",
    "print(\"Training Accuracy :\", clf4.score(xs_train, y_train))\n",
    "print(\"Validation Accuracy :\", clf4.score(xs_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame() \n",
    "for i in range(0,41814):\n",
    "    \n",
    "    df2[i]=X.iloc[i][(X.iloc[i].shift(2) < X.iloc[i]) & (X.iloc[i].shift(-2) < X.iloc[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([  7,  12,  16,  17,  23,  26,  27,  30,  35,  36,  39,  40,  44,\n",
       "             47,  48,  52,  53,  58,  59,  62,  63,  68,  69,  72,  79,  80,\n",
       "             84,  87,  96,  97, 101, 102, 105, 106, 111, 112, 115, 121, 122,\n",
       "            126, 127, 131, 134, 135, 145, 146, 149, 155, 156, 159, 160, 163,\n",
       "            164, 169, 170, 175, 176, 179, 180],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.isna()\n",
    "df2=df2.fillna(0)\n",
    "df2.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2=df2.T\n",
    "df2_copy = X.iloc[0:0,:].copy()\n",
    "for i in [  7,  12,  16,  17,  23,  26,  27,  30,  35,  36,  39,  40,  44,\n",
    "             47,  48,  52,  53,  58,  59,  62,  63,  68,  69,  72,  79,  80,\n",
    "             84,  87,  96,  97, 101, 102, 105, 106, 111, 112, 115, 121, 122,\n",
    "            126, 127, 131, 134, 135, 145, 146, 149, 155, 156, 159, 160, 163,\n",
    "            164, 169, 170, 175, 176, 179, 180]:\n",
    "\n",
    "    df2_copy[i]=X2[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.003408706033912e-68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.335619541867531e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03470161046711315</td>\n",
       "      <td>0.026240115052367056</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.01943209584729744</td>\n",
       "      <td>0.03018274253498323</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.02535214160778556</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009491708355898012</td>\n",
       "      <td>0.011158401538297549</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0137523993119171</td>\n",
       "      <td>0.0116070571917301</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.02345911784677907</td>\n",
       "      <td>0.026702213365201927</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41810</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0136494</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41811</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41812</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41813</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41814</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017897</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41814 rows Ã— 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       1    2    3                      4    5    6    7    8    \\\n",
       "1        0    0    0  8.003408706033912e-68    0    0    0    0   \n",
       "2        0    0    0                      0    0    0    0    0   \n",
       "3        0    0    0                      0    0    0    0    0   \n",
       "4        0    0    0                      0    0    0    0    0   \n",
       "5        0    0    0                      0    0    0    0    0   \n",
       "...    ...  ...  ...                    ...  ...  ...  ...  ...   \n",
       "41810    0    0    0                      0    0    0    0    0   \n",
       "41811    0    0    0                      0    0    0    0    0   \n",
       "41812    0    0    0                      0    0    0    0    0   \n",
       "41813    0    0    0                      0    0    0    0    0   \n",
       "41814    0    0    0                      0    0    0    0    0   \n",
       "\n",
       "                         9    10   ...  171                   172  \\\n",
       "1      9.335619541867531e-07    0  ...    0   0.03470161046711315   \n",
       "2                          0    0  ...    0                     0   \n",
       "3                          0    0  ...    0                     0   \n",
       "4                          0    0  ...    0  0.009491708355898012   \n",
       "5                          0    0  ...    0   0.02345911784677907   \n",
       "...                      ...  ...  ...  ...                   ...   \n",
       "41810                      0    0  ...    0                     0   \n",
       "41811                      0    0  ...    0                     0   \n",
       "41812                      0    0  ...    0                     0   \n",
       "41813                      0    0  ...    0                     0   \n",
       "41814                      0    0  ...    0                     0   \n",
       "\n",
       "                        173 174  175                  176  \\\n",
       "1      0.026240115052367056   0    0  0.01943209584729744   \n",
       "2                         0   0    0                    0   \n",
       "3                         0   0    0  0.02535214160778556   \n",
       "4      0.011158401538297549   0    0   0.0137523993119171   \n",
       "5      0.026702213365201927   0    0                    0   \n",
       "...                     ...  ..  ...                  ...   \n",
       "41810                     0   0    0            0.0136494   \n",
       "41811                     0   0    0                    0   \n",
       "41812                0.1153   0    0                    0   \n",
       "41813                     0   0    0                    0   \n",
       "41814                     0   0    0             0.017897   \n",
       "\n",
       "                       177  178  179 180  \n",
       "1      0.03018274253498323    0    0   0  \n",
       "2                        0    0    0   0  \n",
       "3                        0    0    0   0  \n",
       "4       0.0116070571917301    0    0   0  \n",
       "5                        0    0    0   0  \n",
       "...                    ...  ...  ...  ..  \n",
       "41810                    0    0    0   0  \n",
       "41811                    0    0    0   0  \n",
       "41812                    0    0    0   0  \n",
       "41813                    0    0    0   0  \n",
       "41814                    0    0    0   0  \n",
       "\n",
       "[41814 rows x 180 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_copy=df2_copy.fillna(0)\n",
    "df2_copy=df2_copy.rename(columns=lambda s: s-3)\n",
    "df2_copy.index = np.arange(1, len(df2_copy) + 1)\n",
    "df2_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xss=df2_copy\n",
    "xss_train, xss_test, y_train, y_test = train_test_split(Xss, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.9917187221131537\n",
      "Validation Accuracy : 0.42053772012464674\n"
     ]
    }
   ],
   "source": [
    "#RandomForest\n",
    "clf1 = RandomForestClassifier(oob_score = True,bootstrap=True, max_depth=30, n_estimators=600)\n",
    "clf1.fit(xss_train, y_train)\n",
    "print(\"Training Accuracy :\", clf1.score(xss_train, y_train))\n",
    "print(\"Validation Accuracy :\", clf1.score(xss_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5 Space Group Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z=data1.iloc[:,1].astype(str)\n",
    "x_train, x_test, Z_train, Z_test = train_test_split(X, Z, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.5062645011600928\n",
      "Validation Accuracy : 0.29632582071164576\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "clf4 = SVC(C=20, gamma=0.001, kernel='linear')\n",
    "clf4.fit(x_train, Z_train)\n",
    "print(\"Training Accuracy :\", clf4.score(x_train, Z_train))\n",
    "print(\"Validation Accuracy :\", clf4.score(x_test, Z_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 1.0\n",
      "Validation Accuracy : 0.4586564243785782\n"
     ]
    }
   ],
   "source": [
    "#RandomForest\n",
    "clf1 = RandomForestClassifier(oob_score = True,bootstrap=True, max_depth=30, n_estimators=600)\n",
    "clf1.fit(x_train, Z_train)\n",
    "print(\"Training Accuracy :\", clf1.score(x_train, Z_train))\n",
    "print(\"Validation Accuracy :\", clf1.score(x_test, Z_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6 Discussion\n",
    "\n",
    "In this project, we are trying almost all types of machine learning model to achieve satisfying performance for our classifier of XRD classification. However, the performance for all the models are pretty low. Here are some reasons to hinder our improvement of model:\n",
    "\n",
    "1. overfitting issues\n",
    "We got almost 100% accuracy for RandomForest model and MLP model when we are training small dataset. However, when we apply these model to real dataset, it decreased drastically. Huge overfitting problems can't provide us general features for each class of crystal strucure, leading to failure of classification. Several attempts made to decrease overfitting, like feature pruning and decrease learning depth. Howver, we still get low efficient classification model.\n",
    "\n",
    "2. features can't exhibit unique properties for certain class of crystal\n",
    "We first use 180 features for each sample. If we take all 180 features to describe each sample, it can't exhibt unique properties in a significant way (like peak position/peak area/2theta degree). On the other hand, if we only took peak position as features, it might not provides enough information to achive 14 class classification, leading to poor performance of our model. We tried both method, still can't obtain satisfying accuracy.\n",
    "\n",
    "3. Even though we know the issues presented in my project, there is no clear solution to solve these questions\n",
    "I read current papers to solve current issues. However, current paper use similar (or same method) to achive almost 100% accuracy. But my optimized model only got ~60% when I apply to large dataset. I have no idea on what happened for my model. So I just try to explore any other possible model to achive high performance. However, it still can not work.\n",
    "\n",
    "Here are some ideas on future plan:\n",
    "1. In section 5, we are extracting peak position for each spectrum and get a new dataframe to get training and testing. I believe we could make full use of this dataframe to try to employ any other model to study if we could get better models.\n",
    "\n",
    "2. I tried CNN model with VGG16 like architecture. And my model exhibits low performance. I also construct a multi-layer perceptron like CNN model (in other notebook, not covered in this notebook), still got low performance. I think CNN model would be a good option to do classification problems, I should take more time on model training for CNN model, or I could first visualize spectrum and then use CNN model to do image processing problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
